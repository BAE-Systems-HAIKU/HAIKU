{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"HAIKU (Hybrid AI Integrating Koopman Units) Overview As part of the AI-assisted Climate Tipping Point Modeling (ACTM) program through DARPA, the HAIKU's Climate Model augmentation and analyses (Figure 1) works to develop more accurate predictions of Climate Tipping-Points and their underlying causes. This approach makes use of Koopman Operator Theory. Koopman based models are able to represent complex dynamics with similar fidelity to much larger models with drastic improvements to speed while providing some intrinsic explainability. This approach enables analyses that allow us to identify causal relations between factors of interest in climate systems as well as to point to additional data or interventions that may mitigate undesired climate effects. Figure 1 summarizes the approach. Figure 1: HAIKU Architecture. HAIKU augments conventional climate models with the Hybrid Koopman Climate Model (HKCM) by learning previously umodeled dynamics of error between simulated data and measurements. The Fast Koopman Proxy Model (FKPM) is a learned fully Koopman based proxy model that enables causal analyses and experimentation by this speed-up. Finally, a graphical causal model allows users to visualize and interact with the FKPM and better understand the origin of tipping points and uncertainty in the model. Problem of Interest: Sea Ice Levels in the Arctic The recent decline in Arctic sea-ice coverage, thickness and volume has been dramatic, with the September sea-ice extent declining by about 40% during the past 40 years 1 and sea-ice volume by about 70% over the same time period 2 . Some projections indicate the Arctic climate system will approach the tipping point of an irreversible catastrophic ice-sheet disintegration, sea-level rise, and the disappearance of the perennial sea ice within this century. This tipping point primarily results from Arctic amplification which is believed to be driven by soaring anthropogenic forcing and associated positive feedback loops. Whereas conventional climate models enable impressive gains in understanding and forecasting, they suffer from several shortcomings. Most notably, these include complex physics that are difficult to represent, large computation requirements, and limited explanatory power for emergent behavior, such as tipping points. In the Arctic case, the global climate model (GCM) we propose to use, the Community Earth Science Model (CESM2), requires about 6 hours of a supercomputing cluster per simulated year. Moreover, conventional climate models have limitations with some examples being surface albedo 3 , sensitivity of ice loss to warming 4 , cloud fraction 5 , and cloud radiative forcing 6 . Focusing on one example \u2013 limitation of sensitivity to ice loss warming: with current models of sea ice concentration in the Arctic is that in data, we see a very strong coupling between ocean temperature and sea ice level. But the current climate models are only able to represent this coupling at a much weaker level and generally require much greater temperature increases than observed to generate the observed level of sea ice loss 4 . A model that is able to represent this observed coupling and explain what physics interactions are missed by the current models would be a boon to the field. Additionally, sea ice volume cannot be monitored continuously; it is a challenge to record these measurements in such a remote location. If we are able to assess the value of acquiring new measurements in specific locations at specific times that will have an outsized impact on generating accurate models and predictions, we may be able to improve current models and better quantify and mitigate catastrophic sea ice disintegration in the Arctic. More details about the Global Climate Model and associated data can be found in the Data and Climate Models section alongside the plans for their use in HAIKU. Koopman Modeling The Koopman evolution equation, \u03a8 (t+1) = K \u03a8 (t), is the closed-form climate dynamics equation we use. In a controlled climate system, we describe the evolution of climate states or observables as x (t+1)=F 1 ( x (t), u (t),\u03b8) where t is the time index, x is the climate model state vector, u , is the vector representing climate forcing, and \u03b8 is the vector representing strength of climate interactions. We apply a lifting function to go from the states or observables known to current climate models into a set of collective observables that the Koopman operator can operate on: \u03a8 (t+1)=A(\u03b8) \u03a8 (t)+B(\u03b8) u (t). More details of Koopman Operator Theory and its application to HAIKU can be found int the Koopman Assisted Climate Models section . References 1 \u201cArctic Sea Ice Minimum.\u201d NASA Global Climate Change. (https://climate.nasa.gov/vital-signs/arctic-sea-ice/) accessed 10/09/2021. 2 \u201cPIOMAS Arctic Sea Ice Volume Reanalysis.\u201d Polar Science Center. (http://psc.apl.uw.edu/research/projects/arctic-sea-ice-volume-anomaly/) accessed 10/09/2021. 3 Karlsson, J., and Svensson, G. (2013), Consequences of poor representation of Arctic sea-ice albedo and cloud-radiation interactions in the CMIP5 model ensemble, Geophys. Res. Lett., 40, 4374\u2013 4379, doi:10.1002/grl.50768. 4 Rosenblum, E., and I. Eisenman, 2016: Faster Arctic sea ice retreat in CMIP5 than in CMIP3 due to volcanoes. J. Climate, 29, 9179\u20139188, doi:10.1175/JCLI-D-16-0391.1. 5 Taylor, P. C., Boeke, R. C., Li, Y., and Thompson, D. W. J. Arctic cloud annual cycle biases in climate models. Atmos. Chem. Phys., 19, 8759\u20138782, 2019 https://doi.org/10.5194/acp-19-8759-2019 6 English, J. M., Gettelman, A., and Henderson, G.R. Arctic radiative fluxes: Present-day biases and future projections in CMIP5 models, J. Clim., 28(15), 6019\u2013 6038, doi:10.1175/jcli-d-14-00801.1.","title":"HAIKU (Hybrid AI Integrating Koopman Units)"},{"location":"#haiku-hybrid-ai-integrating-koopman-units","text":"","title":"HAIKU (Hybrid AI Integrating Koopman Units)"},{"location":"#overview","text":"As part of the AI-assisted Climate Tipping Point Modeling (ACTM) program through DARPA, the HAIKU's Climate Model augmentation and analyses (Figure 1) works to develop more accurate predictions of Climate Tipping-Points and their underlying causes. This approach makes use of Koopman Operator Theory. Koopman based models are able to represent complex dynamics with similar fidelity to much larger models with drastic improvements to speed while providing some intrinsic explainability. This approach enables analyses that allow us to identify causal relations between factors of interest in climate systems as well as to point to additional data or interventions that may mitigate undesired climate effects. Figure 1 summarizes the approach. Figure 1: HAIKU Architecture. HAIKU augments conventional climate models with the Hybrid Koopman Climate Model (HKCM) by learning previously umodeled dynamics of error between simulated data and measurements. The Fast Koopman Proxy Model (FKPM) is a learned fully Koopman based proxy model that enables causal analyses and experimentation by this speed-up. Finally, a graphical causal model allows users to visualize and interact with the FKPM and better understand the origin of tipping points and uncertainty in the model.","title":"Overview"},{"location":"#problem-of-interest-sea-ice-levels-in-the-arctic","text":"The recent decline in Arctic sea-ice coverage, thickness and volume has been dramatic, with the September sea-ice extent declining by about 40% during the past 40 years 1 and sea-ice volume by about 70% over the same time period 2 . Some projections indicate the Arctic climate system will approach the tipping point of an irreversible catastrophic ice-sheet disintegration, sea-level rise, and the disappearance of the perennial sea ice within this century. This tipping point primarily results from Arctic amplification which is believed to be driven by soaring anthropogenic forcing and associated positive feedback loops. Whereas conventional climate models enable impressive gains in understanding and forecasting, they suffer from several shortcomings. Most notably, these include complex physics that are difficult to represent, large computation requirements, and limited explanatory power for emergent behavior, such as tipping points. In the Arctic case, the global climate model (GCM) we propose to use, the Community Earth Science Model (CESM2), requires about 6 hours of a supercomputing cluster per simulated year. Moreover, conventional climate models have limitations with some examples being surface albedo 3 , sensitivity of ice loss to warming 4 , cloud fraction 5 , and cloud radiative forcing 6 . Focusing on one example \u2013 limitation of sensitivity to ice loss warming: with current models of sea ice concentration in the Arctic is that in data, we see a very strong coupling between ocean temperature and sea ice level. But the current climate models are only able to represent this coupling at a much weaker level and generally require much greater temperature increases than observed to generate the observed level of sea ice loss 4 . A model that is able to represent this observed coupling and explain what physics interactions are missed by the current models would be a boon to the field. Additionally, sea ice volume cannot be monitored continuously; it is a challenge to record these measurements in such a remote location. If we are able to assess the value of acquiring new measurements in specific locations at specific times that will have an outsized impact on generating accurate models and predictions, we may be able to improve current models and better quantify and mitigate catastrophic sea ice disintegration in the Arctic. More details about the Global Climate Model and associated data can be found in the Data and Climate Models section alongside the plans for their use in HAIKU.","title":"Problem of Interest: Sea Ice Levels in the Arctic"},{"location":"#koopman-modeling","text":"The Koopman evolution equation, \u03a8 (t+1) = K \u03a8 (t), is the closed-form climate dynamics equation we use. In a controlled climate system, we describe the evolution of climate states or observables as x (t+1)=F 1 ( x (t), u (t),\u03b8) where t is the time index, x is the climate model state vector, u , is the vector representing climate forcing, and \u03b8 is the vector representing strength of climate interactions. We apply a lifting function to go from the states or observables known to current climate models into a set of collective observables that the Koopman operator can operate on: \u03a8 (t+1)=A(\u03b8) \u03a8 (t)+B(\u03b8) u (t). More details of Koopman Operator Theory and its application to HAIKU can be found int the Koopman Assisted Climate Models section .","title":"Koopman Modeling"},{"location":"#references","text":"1 \u201cArctic Sea Ice Minimum.\u201d NASA Global Climate Change. (https://climate.nasa.gov/vital-signs/arctic-sea-ice/) accessed 10/09/2021. 2 \u201cPIOMAS Arctic Sea Ice Volume Reanalysis.\u201d Polar Science Center. (http://psc.apl.uw.edu/research/projects/arctic-sea-ice-volume-anomaly/) accessed 10/09/2021. 3 Karlsson, J., and Svensson, G. (2013), Consequences of poor representation of Arctic sea-ice albedo and cloud-radiation interactions in the CMIP5 model ensemble, Geophys. Res. Lett., 40, 4374\u2013 4379, doi:10.1002/grl.50768. 4 Rosenblum, E., and I. Eisenman, 2016: Faster Arctic sea ice retreat in CMIP5 than in CMIP3 due to volcanoes. J. Climate, 29, 9179\u20139188, doi:10.1175/JCLI-D-16-0391.1. 5 Taylor, P. C., Boeke, R. C., Li, Y., and Thompson, D. W. J. Arctic cloud annual cycle biases in climate models. Atmos. Chem. Phys., 19, 8759\u20138782, 2019 https://doi.org/10.5194/acp-19-8759-2019 6 English, J. M., Gettelman, A., and Henderson, G.R. Arctic radiative fluxes: Present-day biases and future projections in CMIP5 models, J. Clim., 28(15), 6019\u2013 6038, doi:10.1175/jcli-d-14-00801.1.","title":"References"},{"location":"analyses/","text":"Analysis Toolkit Our extensible toolkit leverages data from the fast counterfactual simulation of the FKPM (Figure 1). We highlight several planned analyses below, but envision the set of analyses growing to meet the needs of our key analytic questions. FKPM provides the computational engine enabling the analyses, and we leverage HKCM as a validation tool. These components are still in development and will be expanded upon as the implementation is finalized. Figure 1: HAIKU includes an extensible analysis toolkit that leverages the FKPM to run millions of What-Ifs in support of explanatory, exploratory, and quantitative analyses. Semantic Graph Builder The Semantic Graph Builder will build a causal network of key causal factors impacting forecasts, augmenting the set of model factors with user-defined factors computable from the model (e.g., annual variation in sea ice concentration). The FKPM will generate many time series of these factors. We plan to leverage Granger Graphs to apply Granger causality to every pair of factors. This captures the causality between factors given the generated time series. This summary of the climate model will improve understanding by showing predictions and connections in terms of the variables that are most meaningful. Figure 2: Illustration of the Semantic Graph generation which provides a human interpretable representation of climate dynamics in a compact causal graph. The model will be built out by initially calling on the FKPM to generate a large bank of time series for all outputs of interest. This data is then processed by a semi-automated variable transformation which aims to reduce the gridding of the input data to something both meaningful and manageable to a human operator. We anticipate this being mainly manually defined to begin with (eg. spatial averaging over seas, spatial averaging over length scale of interest, etc.). We then compute pairwise Granger causality between variables and leverage LASSO to limit the number of edges and remove edges explained by other pathways. Similarly, we allow for some physics/expert based constraints on causality (eg. spatial proximity requirements). We also envision that the definition of causal variables and discovery of causation between them may be beneficial as a feedback mechanism to tune the parameters of the FKPM itself. Specifically, we may identify regions which could be grouped together in the lifting or eigenfunctions of the FKPM to reduce complexity while maintaining causal prediction power. We anticipate this tool being useful for exploration and understanding of why climate models or climate data behaves as it does. One can quickly vet new models to make sure that the causality present in the Semantic Graph Builder follows physics. Similarly, one can compare a causal model generated from FKPM based on a climate model (CESM) to one on observational data directly to identify potential differences between the two at a high level. Tipping Point Analysis The Tipping Analysis identifies regions of input space where significant, qualitative differences occur in model prediction under small changes in inputs. HAIKU will exploit the linearity of the Koopman Operator to utilize classical results from dynamical systems and control theory, specifically through the eigendecomposition. The eigenvalues identify unstable modes through the sign of the real part. Tipping point analysis uses this eigendecomposition in three separate ways to 1) characterize initial conditions that excite runaway behavior; 2) locate changes in model parameters that introduce new unstable modes; or 3) identify points in time that a control, bounded in magnitude, cannot be designed to counteract an unstable mode. When identifying changes in model parameters or initial conditions that incite runaway behavior, we plan to vary the control values and monitor the change in variables of interest where we may see a tipping point. We can leverage a shooting method (Figure 3) alongside the speed of the FKPM to quickly sample variable space and identify under what conditions tipping points occur and can be mitigated. Figure 3: The shooting method reduces boundary value problems to initial value problems. By modifying a control parameter and running the FKPM forward, we can home in on the boundary value where a tipping point occurs The possible output of such a shooting method can be seen in Figure 4. Figure 4: In this case, we find that the overall sea ice concentration can be modified through certain controls and we can identify values that lead to irrecoverable sea ice loss. Through the use of multiple FKPM trained with appropriate input noise or on different data subsets, we can estimate the uncertainty on both magnitude and timing for when a tipping point will occur using a similar approach. Explainer Explainer augments projection models with traceback information to identify key drivers of quantities of interest in the Semantic Graph. For example, if a projection shows sea ice concentration decreasing, Explainer identifies the pathways through the graph that accounts for that change. Explainer adds interpretability both to \u2018baseline\u2019 model projection as well as What-if runs. The functional details of what will be available in the HAIKU system will be investigated as it is further built out. This is viewed as an optional component as this information can be viewed directly by interrogating the causal model itself. We intend to keep the model at a manageable size so this function is likely not needed directly. Value of New Data Estimator (VoNDE) The Value of New Data Estimator will estimate the value of new data and help identify where to focus resources for future data collection. To improve accuracy of climate forecasts and tipping point estimates, HAIKU assesses how new data will contribute to new, previously unseen model parameters or reduces the uncertainly of target specific downstream effects. Either case indicates the new data are valuable. VoNDE approach We can identify potential measurements by tracing the causality of high variability target forecasts back through the structure of the FKPM cross-terms and the Causal Model to identify important climate variables. These suggest candidate measurements that are likely to be high value, but further analysis is required to verify the effect increased fidelity of these measurements would have on the target forecast. There are a few types of new measurements we consider: Increased accuracy of current measurements : We analyze how the output of the model varies as we add noise to the training data. We rapidly train multiple FKPMs given different sets of noisy training data and quantify how much improvement there is on various model metrics. Increased temporal resolution of measurements : We analyze how the output of the model varies as we change the frequency of measurements. We rapidly train multiple FKPMs given different frequency of training data for the measurement class of interest and quantify how much improvement there is on various model metrics. Increased spatial resolution : We analysis how the output of the model varies as we change the spatial grid resolution for measurements or add in a new measurement at a specific location. We rapidly train multiple FKPMs given different spatial resolution of the training data for the measurement class of interest and quantify how much improvement there is on various model metrics. Increased precision of model parameters/initial conditions : We analyze how the output of the model varies as we shift the initial values and model parameters (such as forcing terms) within their measured uncertainty -- rerunning the FKPM many times and quantifying how the model metrics (mainly variability in output) change with respect to the variance in the model parameters or initial conditions. We then evaluate specific metrics against the above list of experiments to quantify the value of new data: Forecasting accuracy : Do the new measurements increase the accuracy of the modeling forecast with respect to the observed data for the target climate variables of interest? Forecasting robustness : Do the new measurements increase the robustness of the modeling forecast to perturbations in the initial conditions or model parameters? Tipping point accuracy : Do the new measurements increase the accuracy of the magnitude and timing of a forecasted tipping point? Tipping point robustness : Do the new measurements increase the robustness of the magnitude and timing of a forecasted tipping point to perturbations in the initial conditions or model parameters? Causal Robustness : Do the new measurements significantly alter the causal links discovered between climate variables of interest when comparing different FKPMs? These Value of New Data analyses can then be compared with the expected cost of obtaining additional measurements or reducing uncertainty on existing measurements. This is a necessary capability to properly determine how best to invest funds in climate research. VoNDE datasets We have identified several potential measurements that could improve modeling of sea ice concentration in the Arctic that are known to be relevant to the physics in question while being poorly estimated from observed data. If we are able to quantify the improvement to forecasting ability of sea ice, particularly related to tipping points, we can then weigh the cost of collecting more measurements or developing better methods of estimating those measurements from current data. Specifically, cloud cover measurements, under ice water temperature measurements, and atmospheric heat flux measurements are poorly estimated and rarely directly measured for the Arctic region while likely having a large impact on the dynamics of sea ice concentration. We will focus on cloud cover measurements for their potential to improve the sea ice concentration models in Phase II of DARPA ACTM. Details can be found here Additionally, we will use the Atmospheric Temperature datasets currently in the HAIKU analysis to validate our approach by reducing the fidelity of the dataset. Why cloud cover? In the Arctic climate system, physical properties of clouds, such as amount, height, optical thickness, size of cloud droplets and phase partitioning are known to be key factors in determining the surface heat budget over a broad range of time scales due to their radiative effects. These properties display distinct seasonal variations and are subject to complex interactions with the atmosphere, ocean, sea ice, aerosols and large-scale circulation. A better understanding of the role of the summer low-level cloud in shaping sea ice is particularly critical since low-level clouds (bases <3 km) have a greater impact on the Arctic surface energy budget than clouds at higher levels, owing to their proximity to the surface, frequent occurrence, higher emission temperatures, as well as the possibility that they are more often composed of supercooled liquid. These features make summertime low-level clouds more effective at emitting downwelling longwave radiation, while high reflectivity of low-level clouds can also alter downwelling shortwave through changing cloud transmittance and multiple-reflection between the surface and low clouds. Thus, summertime low-level clouds are one of the most important factors in the Arctic climate system since they can effectively modulate the net surface radiation and consequently the rate of summer sea ice melt. Validation datasets We intend to apply the VoNDE approach to determine the value of measurements not currently available. As such, we do not have an adequate dataset to validate the VoNDE approach on cloud cover directly. We will use the Atmospheric Temperature datasets (CESM and NSIDC) currently in use by HAIKU as a validation dataset instead. Specifically, we will train the HAIKU models with degraded versions of the validation dataset (add noise to each measurement, reduce spatial resolution of measurements, reduce temporal resolution of measurements). We then use the HAIKU rapid what-ifs generated by the FKPMs to estimate the reduction of uncertainty on tipping points and model accuracy if we reduce the improved the validation measurements in ways corresponding to the full dataset. We can then finally evaluate the Value of New Data estimation on this dataset. By measuring the accuracy of our VoNDE estimates across a range of \u201cnew\u201d measurements, we can validate the approach and roughly estimate the accuracy of the VoNDE when applied to potential new measurements, such as improvements to cloud cover measurements.","title":"Analysis Toolkit"},{"location":"analyses/#analysis-toolkit","text":"Our extensible toolkit leverages data from the fast counterfactual simulation of the FKPM (Figure 1). We highlight several planned analyses below, but envision the set of analyses growing to meet the needs of our key analytic questions. FKPM provides the computational engine enabling the analyses, and we leverage HKCM as a validation tool. These components are still in development and will be expanded upon as the implementation is finalized. Figure 1: HAIKU includes an extensible analysis toolkit that leverages the FKPM to run millions of What-Ifs in support of explanatory, exploratory, and quantitative analyses.","title":"Analysis Toolkit"},{"location":"analyses/#semantic-graph-builder","text":"The Semantic Graph Builder will build a causal network of key causal factors impacting forecasts, augmenting the set of model factors with user-defined factors computable from the model (e.g., annual variation in sea ice concentration). The FKPM will generate many time series of these factors. We plan to leverage Granger Graphs to apply Granger causality to every pair of factors. This captures the causality between factors given the generated time series. This summary of the climate model will improve understanding by showing predictions and connections in terms of the variables that are most meaningful. Figure 2: Illustration of the Semantic Graph generation which provides a human interpretable representation of climate dynamics in a compact causal graph. The model will be built out by initially calling on the FKPM to generate a large bank of time series for all outputs of interest. This data is then processed by a semi-automated variable transformation which aims to reduce the gridding of the input data to something both meaningful and manageable to a human operator. We anticipate this being mainly manually defined to begin with (eg. spatial averaging over seas, spatial averaging over length scale of interest, etc.). We then compute pairwise Granger causality between variables and leverage LASSO to limit the number of edges and remove edges explained by other pathways. Similarly, we allow for some physics/expert based constraints on causality (eg. spatial proximity requirements). We also envision that the definition of causal variables and discovery of causation between them may be beneficial as a feedback mechanism to tune the parameters of the FKPM itself. Specifically, we may identify regions which could be grouped together in the lifting or eigenfunctions of the FKPM to reduce complexity while maintaining causal prediction power. We anticipate this tool being useful for exploration and understanding of why climate models or climate data behaves as it does. One can quickly vet new models to make sure that the causality present in the Semantic Graph Builder follows physics. Similarly, one can compare a causal model generated from FKPM based on a climate model (CESM) to one on observational data directly to identify potential differences between the two at a high level.","title":"Semantic Graph Builder"},{"location":"analyses/#tipping-point-analysis","text":"The Tipping Analysis identifies regions of input space where significant, qualitative differences occur in model prediction under small changes in inputs. HAIKU will exploit the linearity of the Koopman Operator to utilize classical results from dynamical systems and control theory, specifically through the eigendecomposition. The eigenvalues identify unstable modes through the sign of the real part. Tipping point analysis uses this eigendecomposition in three separate ways to 1) characterize initial conditions that excite runaway behavior; 2) locate changes in model parameters that introduce new unstable modes; or 3) identify points in time that a control, bounded in magnitude, cannot be designed to counteract an unstable mode. When identifying changes in model parameters or initial conditions that incite runaway behavior, we plan to vary the control values and monitor the change in variables of interest where we may see a tipping point. We can leverage a shooting method (Figure 3) alongside the speed of the FKPM to quickly sample variable space and identify under what conditions tipping points occur and can be mitigated. Figure 3: The shooting method reduces boundary value problems to initial value problems. By modifying a control parameter and running the FKPM forward, we can home in on the boundary value where a tipping point occurs The possible output of such a shooting method can be seen in Figure 4. Figure 4: In this case, we find that the overall sea ice concentration can be modified through certain controls and we can identify values that lead to irrecoverable sea ice loss. Through the use of multiple FKPM trained with appropriate input noise or on different data subsets, we can estimate the uncertainty on both magnitude and timing for when a tipping point will occur using a similar approach.","title":"Tipping Point Analysis"},{"location":"analyses/#explainer","text":"Explainer augments projection models with traceback information to identify key drivers of quantities of interest in the Semantic Graph. For example, if a projection shows sea ice concentration decreasing, Explainer identifies the pathways through the graph that accounts for that change. Explainer adds interpretability both to \u2018baseline\u2019 model projection as well as What-if runs. The functional details of what will be available in the HAIKU system will be investigated as it is further built out. This is viewed as an optional component as this information can be viewed directly by interrogating the causal model itself. We intend to keep the model at a manageable size so this function is likely not needed directly.","title":"Explainer"},{"location":"analyses/#value-of-new-data-estimator-vonde","text":"The Value of New Data Estimator will estimate the value of new data and help identify where to focus resources for future data collection. To improve accuracy of climate forecasts and tipping point estimates, HAIKU assesses how new data will contribute to new, previously unseen model parameters or reduces the uncertainly of target specific downstream effects. Either case indicates the new data are valuable.","title":"Value of New Data Estimator (VoNDE)"},{"location":"analyses/#vonde-approach","text":"We can identify potential measurements by tracing the causality of high variability target forecasts back through the structure of the FKPM cross-terms and the Causal Model to identify important climate variables. These suggest candidate measurements that are likely to be high value, but further analysis is required to verify the effect increased fidelity of these measurements would have on the target forecast. There are a few types of new measurements we consider: Increased accuracy of current measurements : We analyze how the output of the model varies as we add noise to the training data. We rapidly train multiple FKPMs given different sets of noisy training data and quantify how much improvement there is on various model metrics. Increased temporal resolution of measurements : We analyze how the output of the model varies as we change the frequency of measurements. We rapidly train multiple FKPMs given different frequency of training data for the measurement class of interest and quantify how much improvement there is on various model metrics. Increased spatial resolution : We analysis how the output of the model varies as we change the spatial grid resolution for measurements or add in a new measurement at a specific location. We rapidly train multiple FKPMs given different spatial resolution of the training data for the measurement class of interest and quantify how much improvement there is on various model metrics. Increased precision of model parameters/initial conditions : We analyze how the output of the model varies as we shift the initial values and model parameters (such as forcing terms) within their measured uncertainty -- rerunning the FKPM many times and quantifying how the model metrics (mainly variability in output) change with respect to the variance in the model parameters or initial conditions. We then evaluate specific metrics against the above list of experiments to quantify the value of new data: Forecasting accuracy : Do the new measurements increase the accuracy of the modeling forecast with respect to the observed data for the target climate variables of interest? Forecasting robustness : Do the new measurements increase the robustness of the modeling forecast to perturbations in the initial conditions or model parameters? Tipping point accuracy : Do the new measurements increase the accuracy of the magnitude and timing of a forecasted tipping point? Tipping point robustness : Do the new measurements increase the robustness of the magnitude and timing of a forecasted tipping point to perturbations in the initial conditions or model parameters? Causal Robustness : Do the new measurements significantly alter the causal links discovered between climate variables of interest when comparing different FKPMs? These Value of New Data analyses can then be compared with the expected cost of obtaining additional measurements or reducing uncertainty on existing measurements. This is a necessary capability to properly determine how best to invest funds in climate research.","title":"VoNDE approach"},{"location":"analyses/#vonde-datasets","text":"We have identified several potential measurements that could improve modeling of sea ice concentration in the Arctic that are known to be relevant to the physics in question while being poorly estimated from observed data. If we are able to quantify the improvement to forecasting ability of sea ice, particularly related to tipping points, we can then weigh the cost of collecting more measurements or developing better methods of estimating those measurements from current data. Specifically, cloud cover measurements, under ice water temperature measurements, and atmospheric heat flux measurements are poorly estimated and rarely directly measured for the Arctic region while likely having a large impact on the dynamics of sea ice concentration. We will focus on cloud cover measurements for their potential to improve the sea ice concentration models in Phase II of DARPA ACTM. Details can be found here Additionally, we will use the Atmospheric Temperature datasets currently in the HAIKU analysis to validate our approach by reducing the fidelity of the dataset.","title":"VoNDE datasets"},{"location":"analyses/#why-cloud-cover","text":"In the Arctic climate system, physical properties of clouds, such as amount, height, optical thickness, size of cloud droplets and phase partitioning are known to be key factors in determining the surface heat budget over a broad range of time scales due to their radiative effects. These properties display distinct seasonal variations and are subject to complex interactions with the atmosphere, ocean, sea ice, aerosols and large-scale circulation. A better understanding of the role of the summer low-level cloud in shaping sea ice is particularly critical since low-level clouds (bases <3 km) have a greater impact on the Arctic surface energy budget than clouds at higher levels, owing to their proximity to the surface, frequent occurrence, higher emission temperatures, as well as the possibility that they are more often composed of supercooled liquid. These features make summertime low-level clouds more effective at emitting downwelling longwave radiation, while high reflectivity of low-level clouds can also alter downwelling shortwave through changing cloud transmittance and multiple-reflection between the surface and low clouds. Thus, summertime low-level clouds are one of the most important factors in the Arctic climate system since they can effectively modulate the net surface radiation and consequently the rate of summer sea ice melt.","title":"Why cloud cover?"},{"location":"analyses/#validation-datasets","text":"We intend to apply the VoNDE approach to determine the value of measurements not currently available. As such, we do not have an adequate dataset to validate the VoNDE approach on cloud cover directly. We will use the Atmospheric Temperature datasets (CESM and NSIDC) currently in use by HAIKU as a validation dataset instead. Specifically, we will train the HAIKU models with degraded versions of the validation dataset (add noise to each measurement, reduce spatial resolution of measurements, reduce temporal resolution of measurements). We then use the HAIKU rapid what-ifs generated by the FKPMs to estimate the reduction of uncertainty on tipping points and model accuracy if we reduce the improved the validation measurements in ways corresponding to the full dataset. We can then finally evaluate the Value of New Data estimation on this dataset. By measuring the accuracy of our VoNDE estimates across a range of \u201cnew\u201d measurements, we can validate the approach and roughly estimate the accuracy of the VoNDE when applied to potential new measurements, such as improvements to cloud cover measurements.","title":"Validation datasets"},{"location":"data_models/","text":"Data and Climate models The HAIKU program will focus on analyzing decade-scale predictions of Arctic sea ice concentrations. We plan to initially use the pre-generated data across a variety of CICE4 parameters to initially construct the HAIKU models, but expect to need to run the CICE4 model eventually before transitioning to the full CESM model. We also plan to leverage real measurement data to train our Hybrid Koopman-Climate Model which will learn to model dynamics present in real data that is not present in the models and their associated pre-generated data. To enable rapid early results and validate our approach, we begin with a stand-alone sea ice model: the Los Alamos sea ice model CICE4 and associated documentation . At the midpoint of Phase 1, we plan to move to the full Community Earth System Model (CESM2) to better model the coupled climate variability. CESM2 integrates CICE5 along with atmosphere (CAM6), ocean (POP2), land (CLM5), and ice sheet (CISM2) models of the NCAR modeling framework. Initial Climate Models and Datasets The CICE5 model requires atmospheric data, including: monthly downward shortwave radiation data at the surface, precipitation, cloud fraction, and four times daily data of 2m air temperature, 2m specific humidity, 10m wind, and 10m air density. Most of the atmospheric data will be derived from the ERA5 reanalysis accessible via the ECWMF data portal ( ERA5 ). The CICE5 model also requires oceanic data, including: monthly sea surface temperature, sea surface salinity, sea surface height, mixed layer depth, and ocean currents. The oceanic forcing will come from several indicators available at the ECMWF Ocean Reanalysis System 5 ( ORAS5 via ECWMF ). In addition, the CICE5 model requires data for vertical heat transport from the deep ocean, obtainable as CESM model output available via NCAR climate data gateway . The first step is to confirm sensitivity observations concluded by the climatology community. Before investigative sensitivity, we will first need to find a correspondence between model outputs and observational data for validation. Observational data for sea ice concentration can be obtained from the National Snow and Ice Data Center (NSIDC). We will need to work in a common projection and resolution to avoid interpolation. Model Prediction - CESM Large Ensemble Project The models are computationally expensive to run. Instead, we look at the CESM Large Ensemble Community Project. This project has produced a publicly available set of climate model simulations performed with the nominal 1-degree latitude/longitude version of the Community Earth System Model version 1 (CESM1) with CAM5.2 as its atmospheric component CESM data . Figure 1: Sea ice concentration percentage in the northern hemisphere generated from the CESM Large Ensemble Community Project. The data is shown in longitude-latitude coordinates. Currently from the Monthly CESM Large Ensemble Community Project [KA15] , we extract several variables that we expect to be causally linked to Sea Ice concentration in the Arctic: ICEFRAC : the Sea Ice coverage fraction over each spatial grid element TS : the atmospheric surface temperature SST : the sea surface temperature HI : the sea ice thickness We have also begun including forcing terms into the model. These measurements and their impact on other observables are learned when the Koopman models are trained, but then when the KoopmanModel is used to forecast forward, a predefined time-series of the forcing terms can be passed in. Currently CO2 is treated as a forcing term and we use historically measured forcing and predicted CO2 based on anthropogenic mechanisms. This allows analysis of how reducing C02 can affect the FKPM predictions for Sea Ice concentration in the Arctic. A preliminary analysis can be found in the initial results section. More detail of the implementation of forcing terms in the Koopman model, can be found in the Koopman Modeling section. Observational Data - NSIDC Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 1 | National Snow and Ice Data Center (nsidc.org) We compare the results of the simulations with observational sea ice concentration data obtained from the National Snow and Ice Data Center. These data include gridded daily (every other day for SMMR data) and monthly averaged sea ice concentrations for both the north and south polar regions. The dataset includes data since 26 October 1978. The data shows the brightness temperature data derived from a few different sensors (microwave radiometers that sense emitted microwave radiation) that represent the sea ice concentration. The data are provided in the polar stereographic projection at a grid cell size of 25 x 25 km. For grid cell sizes of 25 x 25km, the pixel intensities represent the fractional amount of sea ice covering that cell (scaled by 250). Figure 2: Sea ice concentrations in the northern and southern hemisphere from satellite data obtained from NSIDC. The data is shown in a polar projection and 250 on the color scale corresponds to 100% coverage. Currently from the Monthly NSIDC datasets, we extract several variables that we expect to be causally linked to Sea Ice concentration in the Arctic: seaice_conc : the Sea Ice coverage fraction over each spatial grid element (NSIDC G02202_V3) [WA19] T2M : Average Air Temperature at each spatial grid point 2 meters above sea surface (ERA5) [He19] SST : Sea Surface Temperature: ocean temperature at surface of water (ORAS5) [Zu19] Sea Ice Thickness : Sea Ice Thickness (PIOMAS v2.1) [SC11, ZH03] Cloud Cover Data - to support Value of New Data Estimator In Phase II of the ACTM program, we aim to determine the value of potential new measurements that will have an outsized impact on improving the model forecasting accuracy, identification and characterization of tipping points, or improve model robustness. If we are able to quantify the improvement to the HAIKU models of sea ice, particularly related to tipping points, we can then weigh the cost of collecting more measurements or developing better methods of estimating those measurements from current data. Specifically, cloud cover measurements, under ice water temperature measurements, and atmospheric heat flux measurements are poorly estimated and rarely directly measured for the arctic region while likely having a large impact on the dynamics of sea ice concentration. We will focus on cloud cover measurements for their potential to improve the sea ice concentration models in Phase II of DARPA ACTM. Further motivation of this choice and the associated analyses can be found in the Value of New Data Estimator section of this document. Considering the pros and cons of each dataset, we will primarily use cloud and surface heat flux data from ERA5 (1979-2022), MERRA-2 (1980-2022), Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO, 2006-2016), and Clouds and Earth's Radiant Energy System Energy Balanced And Filled (CERES-EBAF Ed4.0, 2006-2016) in ACTM Phase II to include cloud radiative influences in shaping the atmosphere-sea ice connection. ERA5 and MERRA-2 are considered the two best reanalysis products incorporating all available satellite and in-situ information and using the most updated 4- and 3-dimension assimilation schemes, respectively. Collecting accurate measurements of cloud cover are challenging and costly: Most cloud cover measurements are derived from satelite imagery which is insufficient to estimate the elevation of the clouds accurately. Ship based measurements are far more accurate, but are incredibly costly to procure. Station data can be used as a cross-comparison, but is limited to regions on land or at least very near a station. Additionally, procuring measurements can be difficult within the zone of influence of certain countries. By accurately assessing the value of new measurements with specific locations, temporal frequencies, and specified precision can provide the needed context to properly balance the direct cost of making the measurements with the improvement to climate forecasting of Sea Ice tipping points in the Arctic region. Dataset Access Instructions to download and preprocess datasets currently in use on HAIKU can be found on the HAIKU github . This page will continue to be updated as the HAIKU code is built out, validated, and approved for public use and will contain instructions to get HAIKU operational on your system. Data Citations CESM Large Ensemble data [KA15] Kay, J. E., Deser, C., Phillips, A., Mai, A., Hannay, C., Strand, G., Arblaster, J., Bates, S., Danabasoglu, G., Edwards, J., Holland, M. Kushner, P., Lamarque, J.-F., Lawrence, D., Lindsay, K., Middleton, A., Munoz, E., Neale, R., Oleson, K., Polvani, L., and M. Vertenstein (2015), The Community Earth System Model (CESM) Large Ensemble Project: A Community Resource for Studying Climate Change in the Presence of Internal Climate Variability, Bulletin of the American Meteorological Society, doi: 10.1175/BAMS-D-13-00255.1, 96, 1333-1349 NSIDC Sea Ice Concentration data [WA19] Walsh, J. E., W. L. Chapman, F. Fetterer, and J. S. Stewart. 2019. Gridded Monthly Sea Ice Extent and Concentration, 1850 Onward, Version 2. [NSIDC G02202_V3]. Boulder, Colorado USA. NSIDC: National Snow and Ice Data Center. doi: https://doi.org/10.7265/jj4s-tq79. [Date Accessed]. ERA5 Monthly [HE19] Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Hor\u00e1nyi, A., Mu\u00f1oz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Th\u00e9paut, J-N. (2019): ERA5 monthly averaged data on single levels from 1979 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). (Accessed on < DD-MMM-YYYY >), 10.24381/cds.f17050d7 ORAS5 [ZU19] Zuo, H., Balmaseda, M. A., Tietsche, S., Mogensen, K., and Mayer, M.: The ECMWF operational ensemble reanalysis\u2013analysis system for ocean and sea ice: a description of the system and assessment, Ocean Sci., 15, 779\u2013808, https://doi.org/10.5194/os-15-779-2019, 2019. PIOMAS Volume time series and uncertainties [SC11] Schweiger, A., R. Lindsay, J. Zhang, M. Steele, H. Stern, Uncertainty in modeled arctic sea ice volume, J. Geophys. Res., doi:10.1029/2011JC007084, 2011 PIOMAS Model details [ZH03] Zhang, J.L. and D.A. Rothrock, \u201cModeling global sea ice with a thickness and enthalpy distribution model in generalized curvilinear coordinates\u201c, Mon. Weather Rev., 131, 845-861, 2003","title":"Data and Climate models"},{"location":"data_models/#data-and-climate-models","text":"The HAIKU program will focus on analyzing decade-scale predictions of Arctic sea ice concentrations. We plan to initially use the pre-generated data across a variety of CICE4 parameters to initially construct the HAIKU models, but expect to need to run the CICE4 model eventually before transitioning to the full CESM model. We also plan to leverage real measurement data to train our Hybrid Koopman-Climate Model which will learn to model dynamics present in real data that is not present in the models and their associated pre-generated data. To enable rapid early results and validate our approach, we begin with a stand-alone sea ice model: the Los Alamos sea ice model CICE4 and associated documentation . At the midpoint of Phase 1, we plan to move to the full Community Earth System Model (CESM2) to better model the coupled climate variability. CESM2 integrates CICE5 along with atmosphere (CAM6), ocean (POP2), land (CLM5), and ice sheet (CISM2) models of the NCAR modeling framework.","title":"Data and Climate models"},{"location":"data_models/#initial-climate-models-and-datasets","text":"The CICE5 model requires atmospheric data, including: monthly downward shortwave radiation data at the surface, precipitation, cloud fraction, and four times daily data of 2m air temperature, 2m specific humidity, 10m wind, and 10m air density. Most of the atmospheric data will be derived from the ERA5 reanalysis accessible via the ECWMF data portal ( ERA5 ). The CICE5 model also requires oceanic data, including: monthly sea surface temperature, sea surface salinity, sea surface height, mixed layer depth, and ocean currents. The oceanic forcing will come from several indicators available at the ECMWF Ocean Reanalysis System 5 ( ORAS5 via ECWMF ). In addition, the CICE5 model requires data for vertical heat transport from the deep ocean, obtainable as CESM model output available via NCAR climate data gateway . The first step is to confirm sensitivity observations concluded by the climatology community. Before investigative sensitivity, we will first need to find a correspondence between model outputs and observational data for validation. Observational data for sea ice concentration can be obtained from the National Snow and Ice Data Center (NSIDC). We will need to work in a common projection and resolution to avoid interpolation.","title":"Initial Climate Models and Datasets"},{"location":"data_models/#model-prediction-cesm-large-ensemble-project","text":"The models are computationally expensive to run. Instead, we look at the CESM Large Ensemble Community Project. This project has produced a publicly available set of climate model simulations performed with the nominal 1-degree latitude/longitude version of the Community Earth System Model version 1 (CESM1) with CAM5.2 as its atmospheric component CESM data . Figure 1: Sea ice concentration percentage in the northern hemisphere generated from the CESM Large Ensemble Community Project. The data is shown in longitude-latitude coordinates. Currently from the Monthly CESM Large Ensemble Community Project [KA15] , we extract several variables that we expect to be causally linked to Sea Ice concentration in the Arctic: ICEFRAC : the Sea Ice coverage fraction over each spatial grid element TS : the atmospheric surface temperature SST : the sea surface temperature HI : the sea ice thickness We have also begun including forcing terms into the model. These measurements and their impact on other observables are learned when the Koopman models are trained, but then when the KoopmanModel is used to forecast forward, a predefined time-series of the forcing terms can be passed in. Currently CO2 is treated as a forcing term and we use historically measured forcing and predicted CO2 based on anthropogenic mechanisms. This allows analysis of how reducing C02 can affect the FKPM predictions for Sea Ice concentration in the Arctic. A preliminary analysis can be found in the initial results section. More detail of the implementation of forcing terms in the Koopman model, can be found in the Koopman Modeling section.","title":"Model Prediction - CESM Large Ensemble Project"},{"location":"data_models/#observational-data-nsidc","text":"Sea Ice Concentrations from Nimbus-7 SMMR and DMSP SSM/I-SSMIS Passive Microwave Data, Version 1 | National Snow and Ice Data Center (nsidc.org) We compare the results of the simulations with observational sea ice concentration data obtained from the National Snow and Ice Data Center. These data include gridded daily (every other day for SMMR data) and monthly averaged sea ice concentrations for both the north and south polar regions. The dataset includes data since 26 October 1978. The data shows the brightness temperature data derived from a few different sensors (microwave radiometers that sense emitted microwave radiation) that represent the sea ice concentration. The data are provided in the polar stereographic projection at a grid cell size of 25 x 25 km. For grid cell sizes of 25 x 25km, the pixel intensities represent the fractional amount of sea ice covering that cell (scaled by 250). Figure 2: Sea ice concentrations in the northern and southern hemisphere from satellite data obtained from NSIDC. The data is shown in a polar projection and 250 on the color scale corresponds to 100% coverage. Currently from the Monthly NSIDC datasets, we extract several variables that we expect to be causally linked to Sea Ice concentration in the Arctic: seaice_conc : the Sea Ice coverage fraction over each spatial grid element (NSIDC G02202_V3) [WA19] T2M : Average Air Temperature at each spatial grid point 2 meters above sea surface (ERA5) [He19] SST : Sea Surface Temperature: ocean temperature at surface of water (ORAS5) [Zu19] Sea Ice Thickness : Sea Ice Thickness (PIOMAS v2.1) [SC11, ZH03]","title":"Observational Data - NSIDC"},{"location":"data_models/#cloud-cover-data-to-support-value-of-new-data-estimator","text":"In Phase II of the ACTM program, we aim to determine the value of potential new measurements that will have an outsized impact on improving the model forecasting accuracy, identification and characterization of tipping points, or improve model robustness. If we are able to quantify the improvement to the HAIKU models of sea ice, particularly related to tipping points, we can then weigh the cost of collecting more measurements or developing better methods of estimating those measurements from current data. Specifically, cloud cover measurements, under ice water temperature measurements, and atmospheric heat flux measurements are poorly estimated and rarely directly measured for the arctic region while likely having a large impact on the dynamics of sea ice concentration. We will focus on cloud cover measurements for their potential to improve the sea ice concentration models in Phase II of DARPA ACTM. Further motivation of this choice and the associated analyses can be found in the Value of New Data Estimator section of this document. Considering the pros and cons of each dataset, we will primarily use cloud and surface heat flux data from ERA5 (1979-2022), MERRA-2 (1980-2022), Cloud-Aerosol Lidar and Infrared Pathfinder Satellite Observation (CALIPSO, 2006-2016), and Clouds and Earth's Radiant Energy System Energy Balanced And Filled (CERES-EBAF Ed4.0, 2006-2016) in ACTM Phase II to include cloud radiative influences in shaping the atmosphere-sea ice connection. ERA5 and MERRA-2 are considered the two best reanalysis products incorporating all available satellite and in-situ information and using the most updated 4- and 3-dimension assimilation schemes, respectively. Collecting accurate measurements of cloud cover are challenging and costly: Most cloud cover measurements are derived from satelite imagery which is insufficient to estimate the elevation of the clouds accurately. Ship based measurements are far more accurate, but are incredibly costly to procure. Station data can be used as a cross-comparison, but is limited to regions on land or at least very near a station. Additionally, procuring measurements can be difficult within the zone of influence of certain countries. By accurately assessing the value of new measurements with specific locations, temporal frequencies, and specified precision can provide the needed context to properly balance the direct cost of making the measurements with the improvement to climate forecasting of Sea Ice tipping points in the Arctic region.","title":"Cloud Cover Data - to support Value of New Data Estimator"},{"location":"data_models/#dataset-access","text":"Instructions to download and preprocess datasets currently in use on HAIKU can be found on the HAIKU github . This page will continue to be updated as the HAIKU code is built out, validated, and approved for public use and will contain instructions to get HAIKU operational on your system.","title":"Dataset Access"},{"location":"data_models/#data-citations","text":"","title":"Data Citations"},{"location":"data_models/#cesm-large-ensemble-data","text":"[KA15] Kay, J. E., Deser, C., Phillips, A., Mai, A., Hannay, C., Strand, G., Arblaster, J., Bates, S., Danabasoglu, G., Edwards, J., Holland, M. Kushner, P., Lamarque, J.-F., Lawrence, D., Lindsay, K., Middleton, A., Munoz, E., Neale, R., Oleson, K., Polvani, L., and M. Vertenstein (2015), The Community Earth System Model (CESM) Large Ensemble Project: A Community Resource for Studying Climate Change in the Presence of Internal Climate Variability, Bulletin of the American Meteorological Society, doi: 10.1175/BAMS-D-13-00255.1, 96, 1333-1349","title":"CESM Large Ensemble data"},{"location":"data_models/#nsidc-sea-ice-concentration-data","text":"[WA19] Walsh, J. E., W. L. Chapman, F. Fetterer, and J. S. Stewart. 2019. Gridded Monthly Sea Ice Extent and Concentration, 1850 Onward, Version 2. [NSIDC G02202_V3]. Boulder, Colorado USA. NSIDC: National Snow and Ice Data Center. doi: https://doi.org/10.7265/jj4s-tq79. [Date Accessed].","title":"NSIDC Sea Ice Concentration data"},{"location":"data_models/#era5-monthly","text":"[HE19] Hersbach, H., Bell, B., Berrisford, P., Biavati, G., Hor\u00e1nyi, A., Mu\u00f1oz Sabater, J., Nicolas, J., Peubey, C., Radu, R., Rozum, I., Schepers, D., Simmons, A., Soci, C., Dee, D., Th\u00e9paut, J-N. (2019): ERA5 monthly averaged data on single levels from 1979 to present. Copernicus Climate Change Service (C3S) Climate Data Store (CDS). (Accessed on < DD-MMM-YYYY >), 10.24381/cds.f17050d7","title":"ERA5 Monthly"},{"location":"data_models/#oras5","text":"[ZU19] Zuo, H., Balmaseda, M. A., Tietsche, S., Mogensen, K., and Mayer, M.: The ECMWF operational ensemble reanalysis\u2013analysis system for ocean and sea ice: a description of the system and assessment, Ocean Sci., 15, 779\u2013808, https://doi.org/10.5194/os-15-779-2019, 2019.","title":"ORAS5"},{"location":"data_models/#piomas-volume-time-series-and-uncertainties","text":"[SC11] Schweiger, A., R. Lindsay, J. Zhang, M. Steele, H. Stern, Uncertainty in modeled arctic sea ice volume, J. Geophys. Res., doi:10.1029/2011JC007084, 2011","title":"PIOMAS Volume time series and uncertainties"},{"location":"data_models/#piomas-model-details","text":"[ZH03] Zhang, J.L. and D.A. Rothrock, \u201cModeling global sea ice with a thickness and enthalpy distribution model in generalized curvilinear coordinates\u201c, Mon. Weather Rev., 131, 845-861, 2003","title":"PIOMAS Model details"},{"location":"how-to-run-haiku-on-centos7/","text":"HOW TO RUN ON CENTOS 7 OVERVIEW CentOS 7 offers fewer required dependencies in its upstream repositories than Ubuntu, so we need to build some things from scratch. The following steps were followed and verified on a barebones CentOS 7 system. STEPS Installing YUM Dependencies yum install -y epel-release yum update -y yum groupinstall -y \"Development Tools\" yum install -y gcc openssl-devel bzip2-devel libffi-devel zlib-devel netcdf netcdf-devel Installing Python 3.8.10 Note: If you already have Python 3.8.10 installed (whether it's through conda or other means), then skip this step. This step is simply for getting Python running on your system. cd /opt wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz tar xzf Python-3.8.10.tgz cd Python-3.8.10 ./configure --with-optimizations make altinstall ln -s /usr/local/bin/python3.8 /usr/local/bin/python3 You should now confirm your python version with the command: python3 --version Installing CDO from Source The CDO binary is required for Haiku to run. On CentOS 7, we need to build this library from source. We are using CDO version 1.9.9 cd /opt wget https://code.mpimet.mpg.de/attachments/download/23323/cdo-1.9.9.tar.gz tar xzf cdo-1.9.9.tar.gz cd cdo-1.9.9 ./configure --with-netcdf make make install You should now confirm your cdo version with the command: cdo --version Installing Python Dependencies For now, we will be using pip to install python dependencies (Poetry support can be added later) If you are not using conda , then create a python virtual environment and activate it. If you are using conda , you can most likely skip this step (although I have not tested with conda ) python3 -m venv ./venv source ./venv/bin/activate Next, install the pip dependencies using the requirements.txt file in this repo's base directory: pip install -r requirements.txt Configuring PYTHONPATH You need to make sure the project's repository base directory is located on the PYTHONPATH environment variable. For example, if I checkout this repository to /home/test/haiku , then I can accomplish this with the following: export PYTHONPATH=$PYTHONPATH:/home/test I recommend pasting this line at the end of the ~/.bashrc file, so it is run automatically with each new terminal window. Otherwise, you'll need to run it yourself every time you open a new terminal window. Running the Code With the above completed, you should now be able to run the code. First, copy configs/example_config.yml and update the fields accordingly. Then, run training with python scripts/train.py configs/your_config.yml Execution output can be found in the log file specified in the configuration file.","title":"HOW TO RUN ON CENTOS 7"},{"location":"how-to-run-haiku-on-centos7/#how-to-run-on-centos-7","text":"","title":"HOW TO RUN ON CENTOS 7"},{"location":"how-to-run-haiku-on-centos7/#overview","text":"CentOS 7 offers fewer required dependencies in its upstream repositories than Ubuntu, so we need to build some things from scratch. The following steps were followed and verified on a barebones CentOS 7 system.","title":"OVERVIEW"},{"location":"how-to-run-haiku-on-centos7/#steps","text":"","title":"STEPS"},{"location":"how-to-run-haiku-on-centos7/#installing-yum-dependencies","text":"yum install -y epel-release yum update -y yum groupinstall -y \"Development Tools\" yum install -y gcc openssl-devel bzip2-devel libffi-devel zlib-devel netcdf netcdf-devel","title":"Installing YUM Dependencies"},{"location":"how-to-run-haiku-on-centos7/#installing-python-3810","text":"Note: If you already have Python 3.8.10 installed (whether it's through conda or other means), then skip this step. This step is simply for getting Python running on your system. cd /opt wget https://www.python.org/ftp/python/3.8.10/Python-3.8.10.tgz tar xzf Python-3.8.10.tgz cd Python-3.8.10 ./configure --with-optimizations make altinstall ln -s /usr/local/bin/python3.8 /usr/local/bin/python3 You should now confirm your python version with the command: python3 --version","title":"Installing Python 3.8.10"},{"location":"how-to-run-haiku-on-centos7/#installing-cdo-from-source","text":"The CDO binary is required for Haiku to run. On CentOS 7, we need to build this library from source. We are using CDO version 1.9.9 cd /opt wget https://code.mpimet.mpg.de/attachments/download/23323/cdo-1.9.9.tar.gz tar xzf cdo-1.9.9.tar.gz cd cdo-1.9.9 ./configure --with-netcdf make make install You should now confirm your cdo version with the command: cdo --version","title":"Installing CDO from Source"},{"location":"how-to-run-haiku-on-centos7/#installing-python-dependencies","text":"For now, we will be using pip to install python dependencies (Poetry support can be added later) If you are not using conda , then create a python virtual environment and activate it. If you are using conda , you can most likely skip this step (although I have not tested with conda ) python3 -m venv ./venv source ./venv/bin/activate Next, install the pip dependencies using the requirements.txt file in this repo's base directory: pip install -r requirements.txt","title":"Installing Python Dependencies"},{"location":"how-to-run-haiku-on-centos7/#configuring-pythonpath","text":"You need to make sure the project's repository base directory is located on the PYTHONPATH environment variable. For example, if I checkout this repository to /home/test/haiku , then I can accomplish this with the following: export PYTHONPATH=$PYTHONPATH:/home/test I recommend pasting this line at the end of the ~/.bashrc file, so it is run automatically with each new terminal window. Otherwise, you'll need to run it yourself every time you open a new terminal window.","title":"Configuring PYTHONPATH"},{"location":"how-to-run-haiku-on-centos7/#running-the-code","text":"With the above completed, you should now be able to run the code. First, copy configs/example_config.yml and update the fields accordingly. Then, run training with python scripts/train.py configs/your_config.yml Execution output can be found in the log file specified in the configuration file.","title":"Running the Code"},{"location":"koopman/","text":"Koopman Assisted Climate Models The Hybrid AI Koopman-Climate Model (HKCM) aims to augment the ability of current climate models (CICE5 to start) to more accurately model real world measurements by training a Koopman model in series with the climate model to apply missing dynamics at each time step. But from the programmatic level, we have two other components which should fall under this category as well, the Fast Koopman Proxy Model (FKPM) and our Analysis Toolkit. These work together leveraging a Koopman proxy model of the full climate system that can be trained and run in minutes instead of days. This enables our analysis toolkit and allows users to quickly test hypothesis and automatically determine uncertainty on forecasts as well as driving causal factors and potential interventions to avoid tipping points. Across both the HKCM and the FKPM, we will map from the inputs described in Section 3 into Koopman observables space and back. We will also specify the mapping between model parameters (or Koopman observables) and variables of interest described in the same section. By leveraging pregenerated data from the models of interest, we reduce the initial computational cost considerably, while hopefully maintaining sufficient diversity of model parameters to properly train the Koopman operators. Once we\u2019ve validated this approach, we will revisit running the full climate models with specific parameter values as necessary to explore potential tipping points and causal factors. The FKPM and mapping to factors of interest are required for us to present an understandable result and allow for expert interaction through the Analysis Toolkit, which consists of a semantic graph of those factors, an explainer that shows pathways that drive specific factors (like rapid sea ice loss), and an identification of which variables will reach a tipping point (along with further statistics of when, how, and why). More detail into Koopman Operator theory and how the various model components are constructed and serve the climate analyses of interest follow. Koopman Operator Theory Figure 1: Koopman models the dynamics of a reference system (1). Koopman transforms (2) the state space into an observable space and learns a linear operator (3) in the observable space. Various Koopman publications 1,2,3 contain more details. The Koopman Operator (Figure 1) represents the dynamics of a nonlinear, complex, and uncertain system with an infinite-dimensional space (called \u201cobservable space\u201d) that evolves under a linear operator. The lifting function that maps the system into observable space can incorporate prior knowledge, resulting in an abstracted, but understandable representation of the system. The Koopman operator-based compressed linear form enables extremely rapid simulations to explore numerous what-if scenarios and generate data for causal discovery of semantic causal factors. Moreover, specific eigenmodes of the system describe its long run behavior and can be used directly for tipping point analysis. Compared to other data-driven approaches (e.g., neural networks), the Koopman operator extrapolates well. Even though it preserves complex non-linear dynamics, it is linear in observable space and is explainable because the operator can be expressed algebraically relating variables of interest, like ice cover. Additionally, the Koopman operator-based model learns on sparse data. The Koopman evolution equation, \u03a8 (t+1) = K \u03a8 (t), is the closed-form climate dynamics equation we use. In a controlled climate system, we describe the evolution of climate states or observables as x (t+1)=F 1 ( x (t), u (t),\u03b8) where t is the time index, x is the climate model state vector, u , is the vector representing climate forcing, and \u03b8 is the vector representing strength of climate interactions. We apply a lifting function to go from the states or observables known to current climate models into a set of collective observables on which the Koopman operator can act: \u03a8 (t+1)=A(\u03b8) \u03a8 (t)+B(\u03b8) u (t). For the beta software release, the functionality to include forcing terms in both the training and prediction of Koopman models has been added. Equation 1: The Koopman model learns the impact of different inputs on the observables in a linear framework where g are the observables and u are the forcings. Global Climate Models (GCMs) such as the CESM are driven by forcing terms such as greenhouse gases and other anthropogenic factors. We can apply the same forcings to our models to understand the impact of different scenarios on our climate systems. Hybrid AI Koopman-Climate Model (HKCM) We plan to train a Koopman operator-based model to learn the dynamics of the difference between the predictions from current climate models and the actual measured records at each measured time step. This has the potential to identify physics that may be important to the quantification of tipping points or runaway sea-ice loss (Figure 2). Figure 2: Hybrid AI Koopman-Climate Model (HKCM)\u2014The Koopman Operator will predict the dynamics of the system given climate model output for the current step and the estimate of observables from the previous one. HKCM leverages modeled physics in the climate model while accounting for un-modeled physics. By placing the Koopman model in series with the Climate Model, it learns the dynamics of any missing physics and to account for any mismodeled physics. The details of which modes are excited in this Koopman model can be used to trace back to the original climate level variables and help identify which physics properties have been mismodeled. Our initial goal is to verify that the HKCM can learn the missing dynamics in sea ice concentration and to investigate the relevant modes to potentially determine the missing physics interactions in the climate model. Figure 3: Separate FKPMs can be trained to model both the climate model and the observational data. By forcing alignment of their eigenvalues, the two FKPM can be compared directly and a third FKPM can be defined from their difference. This difference can then be used to apply a correction factor directly to the climate model output to bring it in line with the observational data. A preliminary investigation (Figure 4) shows the eigenfunctions of the Mean, Annual, and selected exponential decay modes of the correction-FKPM trained on the difference between CESM1 and NSIDC sea ice concentration data. Figure 4: The resultant modes from a preliminary FKPM to apply a correction to the CESM1 model. The Berents and Kara sea are the main region of discrepancy. We find a missing exponential component with a decay time of 20 years. This is a very preliminary result and is meant simply to be illustrative of how the Koopman mode analysis of these differential models can be helpful in identifying missing physics. This correction-FKPM can likely improve the standard Climate Model's accuracy and an analysis of the FKPM's structure can help identify characteristics of the missing physics contained in the correction-FKPM. For instance, if the FKPM has 3 climate variables and we sea that no correction is needed to predict Air Temperature, but we see that the cross-term that models the impact of Air Temperature onto Sea Ice Concentration is large, then we know something is missing in the original climate model involving the how Air Temperature impacts Sea Ice Concentration. We can further look at the Eigenfunctions to identify spatial regions (perhaps this involves an east to west flow) and the eigenvalues (for what timescales is this relevant?). Fast Koopman Proxy Model (FKPM) We will also train a full Koopman model to create a fast proxy model of the full climate simulation. This will enable a suite of analytics that can extract causality and better characterize tipping points and their associated uncertainty that would take months or years to do with the current best climate models alone. This is shown below (Figure 3). Figure 3: Fast Koopman Proxy Model (FKPM)\u2014The FKPM learns the full dynamics of the HKCM or stand-alone climate model, but is able to operate much faster than the either, enabling the analytic toolkit. Analysis of the eigenfunctions and eigenvalues will help identify tipping points and regions of interest for deeper analysis. Initially, we will train a FKPM using the climate model on its own so we can begin developing the Analysis Toolkit, but as the HKCM becomes more capable, we will train an improved FKPM that leverages it as well and compare the analytics for robustness. 1 Arbabi, H., & Mezic, I. (2017). Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the Koopman operator. SIAM Journal on Applied Dynamical Systems, 16(4):2096\u20132126. 2 Mezic, I. (2005). Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dynamics, 41(1-3), 309-325. 3","title":"Koopman Assisted Climate Models"},{"location":"koopman/#koopman-assisted-climate-models","text":"The Hybrid AI Koopman-Climate Model (HKCM) aims to augment the ability of current climate models (CICE5 to start) to more accurately model real world measurements by training a Koopman model in series with the climate model to apply missing dynamics at each time step. But from the programmatic level, we have two other components which should fall under this category as well, the Fast Koopman Proxy Model (FKPM) and our Analysis Toolkit. These work together leveraging a Koopman proxy model of the full climate system that can be trained and run in minutes instead of days. This enables our analysis toolkit and allows users to quickly test hypothesis and automatically determine uncertainty on forecasts as well as driving causal factors and potential interventions to avoid tipping points. Across both the HKCM and the FKPM, we will map from the inputs described in Section 3 into Koopman observables space and back. We will also specify the mapping between model parameters (or Koopman observables) and variables of interest described in the same section. By leveraging pregenerated data from the models of interest, we reduce the initial computational cost considerably, while hopefully maintaining sufficient diversity of model parameters to properly train the Koopman operators. Once we\u2019ve validated this approach, we will revisit running the full climate models with specific parameter values as necessary to explore potential tipping points and causal factors. The FKPM and mapping to factors of interest are required for us to present an understandable result and allow for expert interaction through the Analysis Toolkit, which consists of a semantic graph of those factors, an explainer that shows pathways that drive specific factors (like rapid sea ice loss), and an identification of which variables will reach a tipping point (along with further statistics of when, how, and why). More detail into Koopman Operator theory and how the various model components are constructed and serve the climate analyses of interest follow.","title":"Koopman Assisted Climate Models"},{"location":"koopman/#koopman-operator-theory","text":"Figure 1: Koopman models the dynamics of a reference system (1). Koopman transforms (2) the state space into an observable space and learns a linear operator (3) in the observable space. Various Koopman publications 1,2,3 contain more details. The Koopman Operator (Figure 1) represents the dynamics of a nonlinear, complex, and uncertain system with an infinite-dimensional space (called \u201cobservable space\u201d) that evolves under a linear operator. The lifting function that maps the system into observable space can incorporate prior knowledge, resulting in an abstracted, but understandable representation of the system. The Koopman operator-based compressed linear form enables extremely rapid simulations to explore numerous what-if scenarios and generate data for causal discovery of semantic causal factors. Moreover, specific eigenmodes of the system describe its long run behavior and can be used directly for tipping point analysis. Compared to other data-driven approaches (e.g., neural networks), the Koopman operator extrapolates well. Even though it preserves complex non-linear dynamics, it is linear in observable space and is explainable because the operator can be expressed algebraically relating variables of interest, like ice cover. Additionally, the Koopman operator-based model learns on sparse data. The Koopman evolution equation, \u03a8 (t+1) = K \u03a8 (t), is the closed-form climate dynamics equation we use. In a controlled climate system, we describe the evolution of climate states or observables as x (t+1)=F 1 ( x (t), u (t),\u03b8) where t is the time index, x is the climate model state vector, u , is the vector representing climate forcing, and \u03b8 is the vector representing strength of climate interactions. We apply a lifting function to go from the states or observables known to current climate models into a set of collective observables on which the Koopman operator can act: \u03a8 (t+1)=A(\u03b8) \u03a8 (t)+B(\u03b8) u (t). For the beta software release, the functionality to include forcing terms in both the training and prediction of Koopman models has been added. Equation 1: The Koopman model learns the impact of different inputs on the observables in a linear framework where g are the observables and u are the forcings. Global Climate Models (GCMs) such as the CESM are driven by forcing terms such as greenhouse gases and other anthropogenic factors. We can apply the same forcings to our models to understand the impact of different scenarios on our climate systems.","title":"Koopman Operator Theory"},{"location":"koopman/#hybrid-ai-koopman-climate-model-hkcm","text":"We plan to train a Koopman operator-based model to learn the dynamics of the difference between the predictions from current climate models and the actual measured records at each measured time step. This has the potential to identify physics that may be important to the quantification of tipping points or runaway sea-ice loss (Figure 2). Figure 2: Hybrid AI Koopman-Climate Model (HKCM)\u2014The Koopman Operator will predict the dynamics of the system given climate model output for the current step and the estimate of observables from the previous one. HKCM leverages modeled physics in the climate model while accounting for un-modeled physics. By placing the Koopman model in series with the Climate Model, it learns the dynamics of any missing physics and to account for any mismodeled physics. The details of which modes are excited in this Koopman model can be used to trace back to the original climate level variables and help identify which physics properties have been mismodeled. Our initial goal is to verify that the HKCM can learn the missing dynamics in sea ice concentration and to investigate the relevant modes to potentially determine the missing physics interactions in the climate model. Figure 3: Separate FKPMs can be trained to model both the climate model and the observational data. By forcing alignment of their eigenvalues, the two FKPM can be compared directly and a third FKPM can be defined from their difference. This difference can then be used to apply a correction factor directly to the climate model output to bring it in line with the observational data. A preliminary investigation (Figure 4) shows the eigenfunctions of the Mean, Annual, and selected exponential decay modes of the correction-FKPM trained on the difference between CESM1 and NSIDC sea ice concentration data. Figure 4: The resultant modes from a preliminary FKPM to apply a correction to the CESM1 model. The Berents and Kara sea are the main region of discrepancy. We find a missing exponential component with a decay time of 20 years. This is a very preliminary result and is meant simply to be illustrative of how the Koopman mode analysis of these differential models can be helpful in identifying missing physics. This correction-FKPM can likely improve the standard Climate Model's accuracy and an analysis of the FKPM's structure can help identify characteristics of the missing physics contained in the correction-FKPM. For instance, if the FKPM has 3 climate variables and we sea that no correction is needed to predict Air Temperature, but we see that the cross-term that models the impact of Air Temperature onto Sea Ice Concentration is large, then we know something is missing in the original climate model involving the how Air Temperature impacts Sea Ice Concentration. We can further look at the Eigenfunctions to identify spatial regions (perhaps this involves an east to west flow) and the eigenvalues (for what timescales is this relevant?).","title":"Hybrid AI Koopman-Climate Model (HKCM)"},{"location":"koopman/#fast-koopman-proxy-model-fkpm","text":"We will also train a full Koopman model to create a fast proxy model of the full climate simulation. This will enable a suite of analytics that can extract causality and better characterize tipping points and their associated uncertainty that would take months or years to do with the current best climate models alone. This is shown below (Figure 3). Figure 3: Fast Koopman Proxy Model (FKPM)\u2014The FKPM learns the full dynamics of the HKCM or stand-alone climate model, but is able to operate much faster than the either, enabling the analytic toolkit. Analysis of the eigenfunctions and eigenvalues will help identify tipping points and regions of interest for deeper analysis. Initially, we will train a FKPM using the climate model on its own so we can begin developing the Analysis Toolkit, but as the HKCM becomes more capable, we will train an improved FKPM that leverages it as well and compare the analytics for robustness. 1 Arbabi, H., & Mezic, I. (2017). Ergodic theory, dynamic mode decomposition, and computation of spectral properties of the Koopman operator. SIAM Journal on Applied Dynamical Systems, 16(4):2096\u20132126. 2 Mezic, I. (2005). Spectral properties of dynamical systems, model reduction and decompositions. Nonlinear Dynamics, 41(1-3), 309-325. 3","title":"Fast Koopman Proxy Model (FKPM)"},{"location":"metrics/","text":"Metrics to evaulate HAIKU capabilities In order to evaluate the utility of the HAIKU models and associated analytics compared to current climate modeling capabilities we plan to use several metrics including measuring the relative accuracy and speed of the Koopman Operator Theory based models, the accuracy/reliability of casual relationships extracted from HAIKU, and the accuracy/reliability of tipping points identified by HAIKU. Accuracy of climate projections FKPM can be trained from observational or simulated data. Initial accuracy metric will compare the relative accuracy of the FKPM (trained on observation) to Climate model's accuracy and that of the climatalogical mean. At present we compute relative error of each point at each time step for data outside of training set. Equation 1: MSE of a model across all spatial observations at a given time step. An example of this accuracy analysis in action can be seen on some preliminary results comparing forecasting from a FKPM trained only on sea ice concentration data to NSIDC data. Figure 1: A FKPM was trained on monthly NSIDC (sea ice concentration only) data from 1997-2000. The Koopman model was then run foreward to forecast monthly predictions from 2001-2004. The pointwise RMSE of the FKPM (blue), CESM1 (orange), and climatalogical mean (green) were then computed from the observed monthly NSIDC data. We plan to average over geographic and temporal step size to better quantify the utility of our models to the longer time scale climate trends as well as to mitigate the potential very small scale noise that the FKPM attempts to simulate. We plan for this metric to evolve as we continue evaluating the HAIKU system: Aggregate spatially before computing the RMSE Compare accuracy of larger scale dynamics to higher resolution Aggregate temporally (or remove annual variation directly) Main goal is to assess decadal scale trends and tipping points, we don\u2019t want model to focus on modeling annual variation when overall trend is more important Compute accuracy per variable/spatial/temporal grids Will enable Phase II ability to identify measurements to improve overall accuracy Robustness of HAIKU models FKPM models are trained on simulated or observational data, both of these sources have measurement uncertainty in the quantities we aim to model. By quantifying the impact variance on these inputs have in the predictions the koopman model is able to generate, we can define a bounds on the uncertainty of the FKPM predictions. Specifically, this is done by training multiple koopman models while varying the training inputs within the bounds of their uncertainty. The speed of the training of the Koopman models allows 10s of models to be trained to get a good estimate of the distribution of Koopman models over the parameters of interest. Figure 2: Software diagram describing the robustness analytic algorithm in action. Analytics of interest include: Robustness to measurement uncertainty Robustness to training data parameter choice (CESM variables or NSIDC versions) Robustness to training window (eg. varying the start/stop of training window by 1 year) Given an FKPM of interest, we can run the above robustness analytics to estimate uncertainty bounds on the forecast of the Koopman models. This will naturally extend itself into the phase 2 value of new data analytic. Proxy Model Speedup The ability of the FKPM to accurately model the climate system is of primary importance, but in order to provide analytics not possible with current climate models, we must also be able to train and test orders of magnitude faster than the current full physics climate simulations. To that end, we propose a metric called Proxy Model Speedup which is the fraction of the time the CESM model takes to evaluate 50 years of climate forecasting at the fidelity described in our datasets section over the time taken for the Koopman model to do the same. Equation 2: Metric to capture speedup of FKPM over standard climate models Measurement of Proxy Model speedup with HAIKU beta version: The measured speedup when leveraging Koopman models allows HAIKU to quickly run many predictions with modified forcing terms, input values, etc to generate the large number of time-series required for causal discovery The fast training time allows us to modify the fidelity/resolution of training data to evaluate the importance of new data (Phase II). Causal Factor Discovery and Confirmation An important analytic in the HAIKU toolkit is to identify causally linked variables. Our current approach at causal discovery involves rapid what-if analyses using the FKPM to tweak potential causal variables and evaluate the change in potential effect variables. We can also identify the off-diagonal terms in the Koopman matrix to infer causality between modes. While our data is high dimensional, we can restrict potential causal links spatially and based on known physics. Once we\u2019ve identified a set of proposed causal links using granger causality in a specific set of FKPMs we then propose a further set of experiments on the original reference climate model via a counterfactual experiment. The accuracy of causal links discovered by the FKPMs is then the fraction of causal links not refuted by high-fidelity models over the total number of proposed causal links. Equation 3: Metric to capture the accuracy of the causal links predicted by the Semantic Graph Generator Where CF is 0 if a causal links disproved by counterfactual evidence or 1 if it is validated and N is the total number of proposed causal links. If we assume that the sampling of CESM model parameters enclose the true observational parameters, measuring the distribution of Causal Link Accuracies of FKPMs trained on various CESM output should provide a reasonable estimate of the Causal Link Accuracy of a FKPM trained to more precisely emulate the observational data. An additional metric to track is the total number of causal relations discovered in the climate system. This will be presented, but we do not have a target value at this point. As the program develops and we present hypothesized causal relations to climate scientists, we expect the usefulness of these causal relations may come into play, but no quantitative metrics are currently planned for this. Tipping Point Accuracy and Consistency FKPM enables what-if analyses orders of magnitude faster than traditional climate models Identify Koopman modes with exponential dynamics (likely to cause runaway behavior) Identify bifurcation by tracking the Koopman eigenvalues Leverage FKPM rapid what-if analyses to identify causal variables that experience runaway dynamics directly Given initial conditions and forcing terms, identify point in time where a bounded control is unable to counteract runaway dynamics Generating a metric to evaluate HAIKU\u2019s ability to accurately predict tipping points is perhaps the most challenging aspect of metric definition. We don\u2019t necessarily have direct observations of tipping points in our observational or simulated data. Identification of key locations and variables driving tipping points will be quantified, but cannot necessarily be verified directly. Specifically, we identify bifurcation by tracking the Koopman eigenvalues and the corresponding Koopman modes; what spatial location and which variables will experience tipping points? We then track the total number and through interactions with climate scientists, how many lead to useful insights or actions. Quantitative metrics are preferred, so we also aim to verify the accuracy of as many proposed tipping points as possible. We can quantify a stochastic (systematic) uncertainty in the tipping point by measuring the magnitude and causal thresholds that lead to the tipping point across several FKPM trained from data generated with different initial values (configurations) of the CESM. For an FKPM trained on climate simulation data, we can go back to the original simulation to verify if a tipping point is present. The FKPM allows us to sample parameter space much more rapidly and the Koopman mode analysis also allows us to identify potential areas to explore. Once we\u2019ve identified potential conditions that trigger a tipping point using our Analysis Toolkit, we return to the original simulations and verify if the predicted tipping point is present in the dynamics of the simulation. This Tipping Point accuracy is the fraction of verified tipping points over the total number of proposed tipping points. Equation 4: Metric to estimate accuracy of tipping points Where VTP (verified tipping point) is 0 if the tipping point is not verified on the current FKPM and 1 if it is. We then range over N FKPMs and over M identified tipping points. Our currently selected observational data doesn\u2019t have large scale tipping points, so as of now, this analysis is restricted to simulated dynamics. But as with our causal analysis accuracy, we expect that the capability of FKPM to model simulated climate models will be similar to its ability to model observational data. And as such, the Tipping Point Accuracy measured on simulated data should be indicative of that on observational data. Additionally, if a specific tipping point is present in several FKPMs derived across a variety of simulation parameters (high tipping point accuracy) and the accuracy of those FKPMs compared to the observational data is high (high model accuracy), we have reasonable support that this tipping point is properly modeled and is likely representative of a tipping point present in the real earth system.","title":"Metrics to evaulate HAIKU capabilities"},{"location":"metrics/#metrics-to-evaulate-haiku-capabilities","text":"In order to evaluate the utility of the HAIKU models and associated analytics compared to current climate modeling capabilities we plan to use several metrics including measuring the relative accuracy and speed of the Koopman Operator Theory based models, the accuracy/reliability of casual relationships extracted from HAIKU, and the accuracy/reliability of tipping points identified by HAIKU.","title":"Metrics to evaulate HAIKU capabilities"},{"location":"metrics/#accuracy-of-climate-projections","text":"FKPM can be trained from observational or simulated data. Initial accuracy metric will compare the relative accuracy of the FKPM (trained on observation) to Climate model's accuracy and that of the climatalogical mean. At present we compute relative error of each point at each time step for data outside of training set. Equation 1: MSE of a model across all spatial observations at a given time step. An example of this accuracy analysis in action can be seen on some preliminary results comparing forecasting from a FKPM trained only on sea ice concentration data to NSIDC data. Figure 1: A FKPM was trained on monthly NSIDC (sea ice concentration only) data from 1997-2000. The Koopman model was then run foreward to forecast monthly predictions from 2001-2004. The pointwise RMSE of the FKPM (blue), CESM1 (orange), and climatalogical mean (green) were then computed from the observed monthly NSIDC data. We plan to average over geographic and temporal step size to better quantify the utility of our models to the longer time scale climate trends as well as to mitigate the potential very small scale noise that the FKPM attempts to simulate. We plan for this metric to evolve as we continue evaluating the HAIKU system: Aggregate spatially before computing the RMSE Compare accuracy of larger scale dynamics to higher resolution Aggregate temporally (or remove annual variation directly) Main goal is to assess decadal scale trends and tipping points, we don\u2019t want model to focus on modeling annual variation when overall trend is more important Compute accuracy per variable/spatial/temporal grids Will enable Phase II ability to identify measurements to improve overall accuracy","title":"Accuracy of climate projections"},{"location":"metrics/#robustness-of-haiku-models","text":"FKPM models are trained on simulated or observational data, both of these sources have measurement uncertainty in the quantities we aim to model. By quantifying the impact variance on these inputs have in the predictions the koopman model is able to generate, we can define a bounds on the uncertainty of the FKPM predictions. Specifically, this is done by training multiple koopman models while varying the training inputs within the bounds of their uncertainty. The speed of the training of the Koopman models allows 10s of models to be trained to get a good estimate of the distribution of Koopman models over the parameters of interest. Figure 2: Software diagram describing the robustness analytic algorithm in action. Analytics of interest include: Robustness to measurement uncertainty Robustness to training data parameter choice (CESM variables or NSIDC versions) Robustness to training window (eg. varying the start/stop of training window by 1 year) Given an FKPM of interest, we can run the above robustness analytics to estimate uncertainty bounds on the forecast of the Koopman models. This will naturally extend itself into the phase 2 value of new data analytic.","title":"Robustness of HAIKU models"},{"location":"metrics/#proxy-model-speedup","text":"The ability of the FKPM to accurately model the climate system is of primary importance, but in order to provide analytics not possible with current climate models, we must also be able to train and test orders of magnitude faster than the current full physics climate simulations. To that end, we propose a metric called Proxy Model Speedup which is the fraction of the time the CESM model takes to evaluate 50 years of climate forecasting at the fidelity described in our datasets section over the time taken for the Koopman model to do the same. Equation 2: Metric to capture speedup of FKPM over standard climate models Measurement of Proxy Model speedup with HAIKU beta version: The measured speedup when leveraging Koopman models allows HAIKU to quickly run many predictions with modified forcing terms, input values, etc to generate the large number of time-series required for causal discovery The fast training time allows us to modify the fidelity/resolution of training data to evaluate the importance of new data (Phase II).","title":"Proxy Model Speedup"},{"location":"metrics/#causal-factor-discovery-and-confirmation","text":"An important analytic in the HAIKU toolkit is to identify causally linked variables. Our current approach at causal discovery involves rapid what-if analyses using the FKPM to tweak potential causal variables and evaluate the change in potential effect variables. We can also identify the off-diagonal terms in the Koopman matrix to infer causality between modes. While our data is high dimensional, we can restrict potential causal links spatially and based on known physics. Once we\u2019ve identified a set of proposed causal links using granger causality in a specific set of FKPMs we then propose a further set of experiments on the original reference climate model via a counterfactual experiment. The accuracy of causal links discovered by the FKPMs is then the fraction of causal links not refuted by high-fidelity models over the total number of proposed causal links. Equation 3: Metric to capture the accuracy of the causal links predicted by the Semantic Graph Generator Where CF is 0 if a causal links disproved by counterfactual evidence or 1 if it is validated and N is the total number of proposed causal links. If we assume that the sampling of CESM model parameters enclose the true observational parameters, measuring the distribution of Causal Link Accuracies of FKPMs trained on various CESM output should provide a reasonable estimate of the Causal Link Accuracy of a FKPM trained to more precisely emulate the observational data. An additional metric to track is the total number of causal relations discovered in the climate system. This will be presented, but we do not have a target value at this point. As the program develops and we present hypothesized causal relations to climate scientists, we expect the usefulness of these causal relations may come into play, but no quantitative metrics are currently planned for this.","title":"Causal Factor Discovery and Confirmation"},{"location":"metrics/#tipping-point-accuracy-and-consistency","text":"FKPM enables what-if analyses orders of magnitude faster than traditional climate models Identify Koopman modes with exponential dynamics (likely to cause runaway behavior) Identify bifurcation by tracking the Koopman eigenvalues Leverage FKPM rapid what-if analyses to identify causal variables that experience runaway dynamics directly Given initial conditions and forcing terms, identify point in time where a bounded control is unable to counteract runaway dynamics Generating a metric to evaluate HAIKU\u2019s ability to accurately predict tipping points is perhaps the most challenging aspect of metric definition. We don\u2019t necessarily have direct observations of tipping points in our observational or simulated data. Identification of key locations and variables driving tipping points will be quantified, but cannot necessarily be verified directly. Specifically, we identify bifurcation by tracking the Koopman eigenvalues and the corresponding Koopman modes; what spatial location and which variables will experience tipping points? We then track the total number and through interactions with climate scientists, how many lead to useful insights or actions. Quantitative metrics are preferred, so we also aim to verify the accuracy of as many proposed tipping points as possible. We can quantify a stochastic (systematic) uncertainty in the tipping point by measuring the magnitude and causal thresholds that lead to the tipping point across several FKPM trained from data generated with different initial values (configurations) of the CESM. For an FKPM trained on climate simulation data, we can go back to the original simulation to verify if a tipping point is present. The FKPM allows us to sample parameter space much more rapidly and the Koopman mode analysis also allows us to identify potential areas to explore. Once we\u2019ve identified potential conditions that trigger a tipping point using our Analysis Toolkit, we return to the original simulations and verify if the predicted tipping point is present in the dynamics of the simulation. This Tipping Point accuracy is the fraction of verified tipping points over the total number of proposed tipping points. Equation 4: Metric to estimate accuracy of tipping points Where VTP (verified tipping point) is 0 if the tipping point is not verified on the current FKPM and 1 if it is. We then range over N FKPMs and over M identified tipping points. Our currently selected observational data doesn\u2019t have large scale tipping points, so as of now, this analysis is restricted to simulated dynamics. But as with our causal analysis accuracy, we expect that the capability of FKPM to model simulated climate models will be similar to its ability to model observational data. And as such, the Tipping Point Accuracy measured on simulated data should be indicative of that on observational data. Additionally, if a specific tipping point is present in several FKPMs derived across a variety of simulation parameters (high tipping point accuracy) and the accuracy of those FKPMs compared to the observational data is high (high model accuracy), we have reasonable support that this tipping point is properly modeled and is likely representative of a tipping point present in the real earth system.","title":"Tipping Point Accuracy and Consistency"},{"location":"software_framework/","text":"Software Framework This is a summary of the structure of the python-based HAIKU software system currently in development. As the system is developed, a beta version will be available on our public github page . This page will include steps required to download and preprocess training data, and to get HAIKU operational on your system. * classes and member functions are denoted as such section. Overall structure Figure 1: Classes representing core functionality in the HAIKU software system. We generate a ClimateData object that encapsulates CESM, NSIDC, or Koopman generated data and converts them all to consistent representations (the same coordinate grid and sets of variables) while storing the provenance as a member. Figure 2: Central objects to the HAIKU system. The analytics causal analysis and analysis toolkit are still being built out and are not present in the current release. The KoopmanModel class contains a trained Koopman model as well as any training parameters associated with the stored data. During training, it takes a single ClimateData object and learns the dynamics of this system. The Predictor object is used to operate on KoopmanModel objects and generate ClimateData with a range of parameters. The Plotter object takes either a KoopmanModel or ClimateData as input, along with runtime parameters, and generates visualizations representing the stored data. As such, this object is also used to visualize predictions. Figure 3: The HAIKU framework ingests data and generates a series of models to enable Tipping Point and other analytics on the climate system. Finally, the KoopmanModel itself or the time-series data contained in a ClimateData object can be passed into the Analytics Toolkit. A CausalModel object is instantiated and can learn a causal structure from time-series data ( ClimateData object) using its internal methods or from the structure of the KoopmanModel itself. Similarly, the CausalModel , ClimateData , or KoopmanModel objects are used as input to different tipping-point analyses inside the toolkit. Figure 4: Leveraging the generated models and time-series data, several analyses are enabled in the Analytics Toolkit. Rounding out the system, there are a variety of metrics that are evaluated either as member functions of the systems or as standalone code. Climate Data ClimateData objects are instantiated by the ClimateDataLoader class which reads in CESM or NSIDC time-series data and converts it to consistent representations (the same coordinate grid and sets of variables). Similarly, ClimateData can also be produced from a KoopmanModel object by running the model through the Predictor object. The time-series data is stored in a numpy array and is by default monthly climate variable data. Internal processing converts between polar and lat-lon coordinates, interpolates missing datapoints, and produces time-series matching the lifted Koopman observables (given a KoopmanModel ). Plotter operates on ClimateData to visually investigate the temporal and spatial behavior of the data. Koopman Models The KoopmanModel class contains a trained Koopman model. The KoopmanModelTrainer class contains functions necessary for training a Koopman model based on provided ClimateData . It is used in conjunction with the Predictor class to generate prediction time-series data in the form of ClimateData . Several model hyperparameters are set at instantiation through the configuration file. The Predictor has a member function, Predictor .predict( _KoopmanModel ,dt), which returns the predicted climate_state after the KoopmanModel has run the original state, x, forward by time dt. This function lifts the original climate state into the Koopman Observables space before propagating the state forward using matrix multiplication, reversing the lifting function, and producing the predicted state in the original climate_state format. There is an associated function for bulk processing of the KoopmanModel . predict_state() function which can provide a full ClimateData object as output. This is more commonly used in most analytics. Currently, the lifting function is a relatively straightforward aggregation of the ClimateData , but we are investigating other approaches as the development continues. The KoopmanModel also has external plotting functions to summarize the model structure including plots of eigenfunctions of selected modes and the distribution of eigenvalues for the KoopmanModel . The forecasting done by the Koopman Models enables the Analytics Toolkit or can produce stand-alone climate forecasts for public consumption. Hybrid Modeling We're still designing the structure of the Hybrid Koopman-Climate Model (HKPM) implementation. For the scope of this project we intend to apply a correction on top of pregenerated data from CESM or another climate model rather than running the full CESM climate model locally and applying the correction in place. This will likely be sufficient to test the HKCM as a proof-of-concept. The HKPM itself is the correction to apply at each time-step of a climate model. Input to this system are two ClimateData time-series with the same variables and over the same time-period. A KoopmanModel object is trained on each of the ClimateData objects constraining them to have the same eigenvalues so that they can be compared directly to one another. The final result is a KoopmanModel which is the difference of these two. This KoopmanModel can then be used directly to provide a correction factor to ClimateData used as input through the KoopmanModel . predict_state() function. Alternatively, it enables analytics (currently done manually) to better understand the causal differences between the two models. It is possible to generate a CausalModel object from this data, which may further enable understanding of the physical difference between the original datasets, but further study is required. Causal Model This class requires ClimateData as well as its own member parameters (which helps define variable transformation from more fine-grained to user oriented causal variables). The CausalModel ._transform_data( ClimateData ) function generates a user oriented ClimateData time-series with many fewer variables. This time-series can then be used as input to train the CausalModel where it uses pairwise Granger Causality coupled with LASSO to limit number of edges, remove edges explained by other pathways. This CausalModel can then be viewed via a CausalModel . print() method. Other analytics are still being considered that may do things like allow a user to request the variable or pathway with the greatest impact on another variable. Analytics Toolkit The initial implementation of the analysis_toolkit hinges around the CausalModel class. The analysis_toolkit class is envisioned to hold many various analysis method, but not to hold any objects itself. Figure 5: Leveraging the generated models and time-series data, several analyses are enabled in the Analytics Toolkit. The analytics toolkit allows for generation of the causal model class and enables the time-series based what-if analyses that can be conducted on the causal variables themselves. The analytics toolkit will also enable the metrics described in the Metrics section. Specifically, a user will be able to select the metric of interest (eg. robustness to training window bounds) and will automatically run the relevant analysis over the specified parameters and present figures and estimates of the metric in question.","title":"Software Framework"},{"location":"software_framework/#software-framework","text":"This is a summary of the structure of the python-based HAIKU software system currently in development. As the system is developed, a beta version will be available on our public github page . This page will include steps required to download and preprocess training data, and to get HAIKU operational on your system. * classes and member functions are denoted as such section. Overall structure Figure 1: Classes representing core functionality in the HAIKU software system. We generate a ClimateData object that encapsulates CESM, NSIDC, or Koopman generated data and converts them all to consistent representations (the same coordinate grid and sets of variables) while storing the provenance as a member. Figure 2: Central objects to the HAIKU system. The analytics causal analysis and analysis toolkit are still being built out and are not present in the current release. The KoopmanModel class contains a trained Koopman model as well as any training parameters associated with the stored data. During training, it takes a single ClimateData object and learns the dynamics of this system. The Predictor object is used to operate on KoopmanModel objects and generate ClimateData with a range of parameters. The Plotter object takes either a KoopmanModel or ClimateData as input, along with runtime parameters, and generates visualizations representing the stored data. As such, this object is also used to visualize predictions. Figure 3: The HAIKU framework ingests data and generates a series of models to enable Tipping Point and other analytics on the climate system. Finally, the KoopmanModel itself or the time-series data contained in a ClimateData object can be passed into the Analytics Toolkit. A CausalModel object is instantiated and can learn a causal structure from time-series data ( ClimateData object) using its internal methods or from the structure of the KoopmanModel itself. Similarly, the CausalModel , ClimateData , or KoopmanModel objects are used as input to different tipping-point analyses inside the toolkit. Figure 4: Leveraging the generated models and time-series data, several analyses are enabled in the Analytics Toolkit. Rounding out the system, there are a variety of metrics that are evaluated either as member functions of the systems or as standalone code.","title":"Software Framework"},{"location":"software_framework/#climate-data","text":"ClimateData objects are instantiated by the ClimateDataLoader class which reads in CESM or NSIDC time-series data and converts it to consistent representations (the same coordinate grid and sets of variables). Similarly, ClimateData can also be produced from a KoopmanModel object by running the model through the Predictor object. The time-series data is stored in a numpy array and is by default monthly climate variable data. Internal processing converts between polar and lat-lon coordinates, interpolates missing datapoints, and produces time-series matching the lifted Koopman observables (given a KoopmanModel ). Plotter operates on ClimateData to visually investigate the temporal and spatial behavior of the data.","title":"Climate Data"},{"location":"software_framework/#koopman-models","text":"The KoopmanModel class contains a trained Koopman model. The KoopmanModelTrainer class contains functions necessary for training a Koopman model based on provided ClimateData . It is used in conjunction with the Predictor class to generate prediction time-series data in the form of ClimateData . Several model hyperparameters are set at instantiation through the configuration file. The Predictor has a member function, Predictor .predict( _KoopmanModel ,dt), which returns the predicted climate_state after the KoopmanModel has run the original state, x, forward by time dt. This function lifts the original climate state into the Koopman Observables space before propagating the state forward using matrix multiplication, reversing the lifting function, and producing the predicted state in the original climate_state format. There is an associated function for bulk processing of the KoopmanModel . predict_state() function which can provide a full ClimateData object as output. This is more commonly used in most analytics. Currently, the lifting function is a relatively straightforward aggregation of the ClimateData , but we are investigating other approaches as the development continues. The KoopmanModel also has external plotting functions to summarize the model structure including plots of eigenfunctions of selected modes and the distribution of eigenvalues for the KoopmanModel . The forecasting done by the Koopman Models enables the Analytics Toolkit or can produce stand-alone climate forecasts for public consumption.","title":"Koopman Models"},{"location":"software_framework/#hybrid-modeling","text":"We're still designing the structure of the Hybrid Koopman-Climate Model (HKPM) implementation. For the scope of this project we intend to apply a correction on top of pregenerated data from CESM or another climate model rather than running the full CESM climate model locally and applying the correction in place. This will likely be sufficient to test the HKCM as a proof-of-concept. The HKPM itself is the correction to apply at each time-step of a climate model. Input to this system are two ClimateData time-series with the same variables and over the same time-period. A KoopmanModel object is trained on each of the ClimateData objects constraining them to have the same eigenvalues so that they can be compared directly to one another. The final result is a KoopmanModel which is the difference of these two. This KoopmanModel can then be used directly to provide a correction factor to ClimateData used as input through the KoopmanModel . predict_state() function. Alternatively, it enables analytics (currently done manually) to better understand the causal differences between the two models. It is possible to generate a CausalModel object from this data, which may further enable understanding of the physical difference between the original datasets, but further study is required.","title":"Hybrid Modeling"},{"location":"software_framework/#causal-model","text":"This class requires ClimateData as well as its own member parameters (which helps define variable transformation from more fine-grained to user oriented causal variables). The CausalModel ._transform_data( ClimateData ) function generates a user oriented ClimateData time-series with many fewer variables. This time-series can then be used as input to train the CausalModel where it uses pairwise Granger Causality coupled with LASSO to limit number of edges, remove edges explained by other pathways. This CausalModel can then be viewed via a CausalModel . print() method. Other analytics are still being considered that may do things like allow a user to request the variable or pathway with the greatest impact on another variable.","title":"Causal Model"},{"location":"software_framework/#analytics-toolkit","text":"The initial implementation of the analysis_toolkit hinges around the CausalModel class. The analysis_toolkit class is envisioned to hold many various analysis method, but not to hold any objects itself. Figure 5: Leveraging the generated models and time-series data, several analyses are enabled in the Analytics Toolkit. The analytics toolkit allows for generation of the causal model class and enables the time-series based what-if analyses that can be conducted on the causal variables themselves. The analytics toolkit will also enable the metrics described in the Metrics section. Specifically, a user will be able to select the metric of interest (eg. robustness to training window bounds) and will automatically run the relevant analysis over the specified parameters and present figures and estimates of the metric in question.","title":"Analytics Toolkit"},{"location":"About/about/","text":"DARPA's ACTM program The DARPA AI-assisted Climate Tipping-point Modeling ( ACTM ) 1 program explores the use of third-wave AI methods to enhance the complex interconnected processes modeling climate change. The main goal is to develop hybrid AI models that combine Artificial Intelligence/Machine Learning (AI/ML) models with conventional physics models of the climate and earth systems that capture missing physical, chemical, or biological processes and identify causal factors with sufficient computational efficiency to explore decadal scale effects and characterize tipping points and bifurcations in these systems. The secondary goal is to develop new methods to assimilate diverse data into models and estimate \"value of new data\" to enhance confidence in target-specific forecasts relative to state-of-the-art techniques. HAIKU Team BAE Systems 2 and AIMdyn, Inc . 3 continue their fruitful collaboration applying Koopman to diverse DoD problems (Gamebreaker, COMBAT, and DITTO). Dr. Michael Planer (BAE Systems) is Principal Investigator, leveraging experience in physical and AI systems. Dr. Maria Fonoberova and Prof. Igor Mezic (AIMdyn) contribute Koopman\u2010based innovations. Prof. Qinghua Ding 4 (AIMdyn) is a climate expert, significantly advancing understanding of the relative contributions of internal and external forcing in Arctic climate variability. 1 Elliott, Joshua. \"AI-assisted Climate Tipping-point Modeling (ACTM).\" DARPA , https://www.darpa.mil/program/ai-assisted-climate-tipping-point-modeling 2 Fast Labs. \"Intelligent Autonomous Systems R&D.\" BAE Systems , https://www.baesystems.com/en/product/autonomy-r-d 3 AIMdyn , https://aimdyn.com/ 4 Ding, Qinghua. UCSB , https://www.geog.ucsb.edu/people/faculty/qinghua-ding","title":"About"},{"location":"About/about/#darpas-actm-program","text":"The DARPA AI-assisted Climate Tipping-point Modeling ( ACTM ) 1 program explores the use of third-wave AI methods to enhance the complex interconnected processes modeling climate change. The main goal is to develop hybrid AI models that combine Artificial Intelligence/Machine Learning (AI/ML) models with conventional physics models of the climate and earth systems that capture missing physical, chemical, or biological processes and identify causal factors with sufficient computational efficiency to explore decadal scale effects and characterize tipping points and bifurcations in these systems. The secondary goal is to develop new methods to assimilate diverse data into models and estimate \"value of new data\" to enhance confidence in target-specific forecasts relative to state-of-the-art techniques.","title":"DARPA's ACTM program"},{"location":"About/about/#haiku-team","text":"BAE Systems 2 and AIMdyn, Inc . 3 continue their fruitful collaboration applying Koopman to diverse DoD problems (Gamebreaker, COMBAT, and DITTO). Dr. Michael Planer (BAE Systems) is Principal Investigator, leveraging experience in physical and AI systems. Dr. Maria Fonoberova and Prof. Igor Mezic (AIMdyn) contribute Koopman\u2010based innovations. Prof. Qinghua Ding 4 (AIMdyn) is a climate expert, significantly advancing understanding of the relative contributions of internal and external forcing in Arctic climate variability. 1 Elliott, Joshua. \"AI-assisted Climate Tipping-point Modeling (ACTM).\" DARPA , https://www.darpa.mil/program/ai-assisted-climate-tipping-point-modeling 2 Fast Labs. \"Intelligent Autonomous Systems R&D.\" BAE Systems , https://www.baesystems.com/en/product/autonomy-r-d 3 AIMdyn , https://aimdyn.com/ 4 Ding, Qinghua. UCSB , https://www.geog.ucsb.edu/people/faculty/qinghua-ding","title":"HAIKU Team"},{"location":"Results/additional_variables_and_forcing/","text":"Preliminary Results Additional Climate Variables as Observables We continue the development of the HAIKU system by including additional climate variables in the FKPM training. In particular, sea (potential) temperature, atmospheric temperature, and sea ice thickness were added. These generally improve the performance of the models and we plan to investigate the inclusion of additional variables (eg. solar irradiance and greenhouse gases). We compare observational-based and CESM-based FKPMs trained on all climate variables. The mean modes are shown in Figure 4 and match expectation for the time average of these variables across the training interval. Figure 4: The mean modes are visualized for the CESM-based FKPM (left) and the observational-based FKPM (right). The results are qualitatively similar and also match physical expectations for the time average of these variables. The annual modes are shown in Figure 5 and match expectation for the typical annual fluctuation of these variables across the training interval. Figure 5: The annual modes are visualized for the CESM-based FKPM (left) and the observational-based FKPM (right). The results are qualitatively similar and also match physical expectations for annual variance of these variables. We have preliminary evidence that shows an improvement in modelling the sea ice concentration dynamics with the inclusion of these additional climate variables. We are interested in assessing the quality of the predictions generated from models with additional variables. The Root Mean Squared Error (RMSE) is computed between the monthly predictions of various models and the NSIDC monthly sea ice concentration data in Figure 6. Figure 6: Four FKPM were trained with differing sets of climate variables (from observational data) and the pointwise RMSE of monthly Sea Ice concentration is shown (blue) compared to CESM1 data (orange) and the climatological mean (green). We see an accuracy gain when including ORAS5 Atmospheric Temperature (right), but not when including ERA5 Sea Surface Temperature (bottom) compared to Sea Ice Concentrations alone (top-left). From this result, it is clear that the FKPM captures the dynamics more accurately with the inclusion of Atmospheric temperature. We speculate that the Sea Surface temperature has little positive impact we cannot obtain the sea surface temperatures in regions that contain sea ice. Including CESM simulated sea (potential) temperatures a few feet below the sea ice as an additional climate variable could improve the accuracy of these models. This will be a nice proof of concept for our Phase II approach to identify new measurements that could improve the accuracy of models built from observational data. Inclusion of forcing terms in the model Global Climate Models (GCMs) such as the Community Earth System Model (CESM) are driven by forcings such as greenhouse gases and other anthropogenic factors. We can apply the same forcings to our model to understand the impact of different scenarios on our climate systems. Figure 7 shows our model\u2019s predictions under two different forcing scenarios: one with constant forcing after 2009, and one with historical forcing. Importantly, we find that the sea ice dynamics are mostly internally driven with little direct impact from the CO2 forcing. Figure 7: \u2013 September sea ice area from 1979 to 2027. (Blue) Predictions from a Koopman model trained from 01/1979 to 12/2009. (Orange) Mean of CESM1 ensemble members\u2019 predictions. (Green) CO2 volume mixing ratio. Two scenarios are shown: constant forcing after 2009 (top) and complete historical forcing (bottom). The red star represents a forecast of the sea ice in September 2022 based on the current sea ice: https://www.arcus.org/sipn/sea-ice-outlook/2022/july","title":"Expanded Inputs"},{"location":"Results/additional_variables_and_forcing/#preliminary-results","text":"","title":"Preliminary Results"},{"location":"Results/additional_variables_and_forcing/#additional-climate-variables-as-observables","text":"We continue the development of the HAIKU system by including additional climate variables in the FKPM training. In particular, sea (potential) temperature, atmospheric temperature, and sea ice thickness were added. These generally improve the performance of the models and we plan to investigate the inclusion of additional variables (eg. solar irradiance and greenhouse gases). We compare observational-based and CESM-based FKPMs trained on all climate variables. The mean modes are shown in Figure 4 and match expectation for the time average of these variables across the training interval. Figure 4: The mean modes are visualized for the CESM-based FKPM (left) and the observational-based FKPM (right). The results are qualitatively similar and also match physical expectations for the time average of these variables. The annual modes are shown in Figure 5 and match expectation for the typical annual fluctuation of these variables across the training interval. Figure 5: The annual modes are visualized for the CESM-based FKPM (left) and the observational-based FKPM (right). The results are qualitatively similar and also match physical expectations for annual variance of these variables. We have preliminary evidence that shows an improvement in modelling the sea ice concentration dynamics with the inclusion of these additional climate variables. We are interested in assessing the quality of the predictions generated from models with additional variables. The Root Mean Squared Error (RMSE) is computed between the monthly predictions of various models and the NSIDC monthly sea ice concentration data in Figure 6. Figure 6: Four FKPM were trained with differing sets of climate variables (from observational data) and the pointwise RMSE of monthly Sea Ice concentration is shown (blue) compared to CESM1 data (orange) and the climatological mean (green). We see an accuracy gain when including ORAS5 Atmospheric Temperature (right), but not when including ERA5 Sea Surface Temperature (bottom) compared to Sea Ice Concentrations alone (top-left). From this result, it is clear that the FKPM captures the dynamics more accurately with the inclusion of Atmospheric temperature. We speculate that the Sea Surface temperature has little positive impact we cannot obtain the sea surface temperatures in regions that contain sea ice. Including CESM simulated sea (potential) temperatures a few feet below the sea ice as an additional climate variable could improve the accuracy of these models. This will be a nice proof of concept for our Phase II approach to identify new measurements that could improve the accuracy of models built from observational data.","title":"Additional Climate Variables as Observables"},{"location":"Results/additional_variables_and_forcing/#inclusion-of-forcing-terms-in-the-model","text":"Global Climate Models (GCMs) such as the Community Earth System Model (CESM) are driven by forcings such as greenhouse gases and other anthropogenic factors. We can apply the same forcings to our model to understand the impact of different scenarios on our climate systems. Figure 7 shows our model\u2019s predictions under two different forcing scenarios: one with constant forcing after 2009, and one with historical forcing. Importantly, we find that the sea ice dynamics are mostly internally driven with little direct impact from the CO2 forcing. Figure 7: \u2013 September sea ice area from 1979 to 2027. (Blue) Predictions from a Koopman model trained from 01/1979 to 12/2009. (Orange) Mean of CESM1 ensemble members\u2019 predictions. (Green) CO2 volume mixing ratio. Two scenarios are shown: constant forcing after 2009 (top) and complete historical forcing (bottom). The red star represents a forecast of the sea ice in September 2022 based on the current sea ice: https://www.arcus.org/sipn/sea-ice-outlook/2022/july","title":"Inclusion of forcing terms in the model"},{"location":"Results/initial_motivation_results/","text":"Preliminary Results We are initially investigating the simulation data from the CESM1 Large Ensemble Community Project (LENS) as well as observational satellite data from the National Snow and Ice Data Center (NSIDC) from 1979 to 2020. All results shown here are based on a Fast Koopman Proxy Models (FKPM). Proof of Concept: Identifying Regions of Interest Unlike most Machine Learning methodologies, the Koopman Operator Theoretical (KOT) framework learns a physically interpretable model. Applying Koopman Mode Decomposition (KMD) reveals underlying low-dimensional dynamics contained in the Koopman modes and eigenvalues. The Koopman modes associated with important eigenvalues can explain the spatial extent of the low-dimensional dynamics. Training a FKPM on NSIDC sea ice concentrations (ICEFRAC) enables identification of spatial regions that experience rapid ice loss as seen in Figure 1. A region of interest is highlighted in the Barents and Kara seas as having a large components of rapid exponential sea ice loss. Figure 1: Koopman modes associated with exponential decay dynamics in the Northern (left) and Southern (right) hemispheres. This FKPM was trained using NSIDC data (sea ice concentration only) from 1979-2020. Initial FKPM Results Figure 2: Koopman modes representing the mean and annual variation in sea ice concentration over five year windows for the Northern hemisphere. (Top) 1979\u20131983 period and (bottom) 2016\u20132020 period. Two FKPM were trained on observational data (left) and CESM ensemble data (right). We see that the extent of the mean and annual modes is qualitatively similar between observational-based and CESM-based FKPMs. Additionally, the regions of high annual variance are highlighted as expected in the annual modes, while the regions of highest sea-ice concentration are highlighted in the mean modes. Next, we move to a more quantitative analysis looking at the accuracy of predictions generated by a FKPM trained from observational data. To visualize the decadal trends of the data and our model, we looked at the September sea ice area over the last 40 years. Figure 3 shows that while the climatological mean is only able to capture the annual variation, our model is able to learn the long-term trend of the sea ice in addition to the annual variation. Figure 3: September sea ice area from 1979 to 2020. (Blue) Predictions from a Koopman model trained from 01/1979 to 12/2009. (Orange) Mean of CESM1 ensemble members\u2019 predictions. (Green) Climatological mean of NSIDC observations. Each calendar month in the climatological mean is the mean of the data values of that calendar month over the entire training interval. The model is unaware of the dynamics of other causal climate variables that directly influence the sea ice concentration dynamics. Expanding the dictionary of observables used in the KOT framework by including additional climate variables will improve the modelling power of the HAIKU system.","title":"Initial Results"},{"location":"Results/initial_motivation_results/#preliminary-results","text":"We are initially investigating the simulation data from the CESM1 Large Ensemble Community Project (LENS) as well as observational satellite data from the National Snow and Ice Data Center (NSIDC) from 1979 to 2020. All results shown here are based on a Fast Koopman Proxy Models (FKPM).","title":"Preliminary Results"},{"location":"Results/initial_motivation_results/#proof-of-concept-identifying-regions-of-interest","text":"Unlike most Machine Learning methodologies, the Koopman Operator Theoretical (KOT) framework learns a physically interpretable model. Applying Koopman Mode Decomposition (KMD) reveals underlying low-dimensional dynamics contained in the Koopman modes and eigenvalues. The Koopman modes associated with important eigenvalues can explain the spatial extent of the low-dimensional dynamics. Training a FKPM on NSIDC sea ice concentrations (ICEFRAC) enables identification of spatial regions that experience rapid ice loss as seen in Figure 1. A region of interest is highlighted in the Barents and Kara seas as having a large components of rapid exponential sea ice loss. Figure 1: Koopman modes associated with exponential decay dynamics in the Northern (left) and Southern (right) hemispheres. This FKPM was trained using NSIDC data (sea ice concentration only) from 1979-2020.","title":"Proof of Concept: Identifying Regions of Interest"},{"location":"Results/initial_motivation_results/#initial-fkpm-results","text":"Figure 2: Koopman modes representing the mean and annual variation in sea ice concentration over five year windows for the Northern hemisphere. (Top) 1979\u20131983 period and (bottom) 2016\u20132020 period. Two FKPM were trained on observational data (left) and CESM ensemble data (right). We see that the extent of the mean and annual modes is qualitatively similar between observational-based and CESM-based FKPMs. Additionally, the regions of high annual variance are highlighted as expected in the annual modes, while the regions of highest sea-ice concentration are highlighted in the mean modes. Next, we move to a more quantitative analysis looking at the accuracy of predictions generated by a FKPM trained from observational data. To visualize the decadal trends of the data and our model, we looked at the September sea ice area over the last 40 years. Figure 3 shows that while the climatological mean is only able to capture the annual variation, our model is able to learn the long-term trend of the sea ice in addition to the annual variation. Figure 3: September sea ice area from 1979 to 2020. (Blue) Predictions from a Koopman model trained from 01/1979 to 12/2009. (Orange) Mean of CESM1 ensemble members\u2019 predictions. (Green) Climatological mean of NSIDC observations. Each calendar month in the climatological mean is the mean of the data values of that calendar month over the entire training interval. The model is unaware of the dynamics of other causal climate variables that directly influence the sea ice concentration dynamics. Expanding the dictionary of observables used in the KOT framework by including additional climate variables will improve the modelling power of the HAIKU system.","title":"Initial FKPM Results"},{"location":"Results/modeling_subregions/","text":"Preliminary Results Additional Climate Variables as Observables We investigated the evolution of sea ice concentration in specific subregions ( this has not yet made it into the publicly released code ). In contrast to applying Koopman methods to data from an entire hemisphere, applying the methods to data from a subregion yields more robust models. When we investigate the Barents Sea subregion, we find that the Koopman methods reveal highly decaying dynamics shown in Figure 2. This is in sharp contrast to applying the methods over a larger spatial region, which reveal dominant stationary dynamics shown in Figure 1. This is not surprising, as the visualizations of the training data in Figures 1 and 2 indicate that the majority of the data in the northern hemisphere is stationary, while most of the data in the Barents Sea subregion is decreasing over time. Figure 1: \u2013 (Left) Koopman methods are applied to data from the entire northern hemisphere. (Middle) 2D plot of the training data from 1978-2020. Each column contains vectorized data from a single point in time. (Right) Koopman eigenvalues extracted from the training data \u2013 colored by their contribution to the dynamics in the training data. Figure 2: \u2013 (Left) Koopman methods are applied to data from the Barents Sea subregion. (Middle) 2D plot of the training data from 1978-2020. Each column contains vectorized data from a single point in time. (Right) Koopman eigenvalues extracted from the training data \u2013 colored by their contribution to the dynamics in the training data. We find correspondences of the Koopman eigenvalues between the models. We see that the decaying dynamics in the Barents Sea region are roughly captured by the model trained on the entire northern hemisphere. The small differences in the learned dynamics are likely due to the fact that the model trained on data from the entire northern hemisphere is subject to more noise than the localized model trained only on data from the Barents Sea subregion. Further investigation into using information from the localized models to improve the global model is currently being conducted. This is part of our ongoing work on improving the robustness of the Koopman models.","title":"Modeling Subregions"},{"location":"Results/modeling_subregions/#preliminary-results","text":"","title":"Preliminary Results"},{"location":"Results/modeling_subregions/#additional-climate-variables-as-observables","text":"We investigated the evolution of sea ice concentration in specific subregions ( this has not yet made it into the publicly released code ). In contrast to applying Koopman methods to data from an entire hemisphere, applying the methods to data from a subregion yields more robust models. When we investigate the Barents Sea subregion, we find that the Koopman methods reveal highly decaying dynamics shown in Figure 2. This is in sharp contrast to applying the methods over a larger spatial region, which reveal dominant stationary dynamics shown in Figure 1. This is not surprising, as the visualizations of the training data in Figures 1 and 2 indicate that the majority of the data in the northern hemisphere is stationary, while most of the data in the Barents Sea subregion is decreasing over time. Figure 1: \u2013 (Left) Koopman methods are applied to data from the entire northern hemisphere. (Middle) 2D plot of the training data from 1978-2020. Each column contains vectorized data from a single point in time. (Right) Koopman eigenvalues extracted from the training data \u2013 colored by their contribution to the dynamics in the training data. Figure 2: \u2013 (Left) Koopman methods are applied to data from the Barents Sea subregion. (Middle) 2D plot of the training data from 1978-2020. Each column contains vectorized data from a single point in time. (Right) Koopman eigenvalues extracted from the training data \u2013 colored by their contribution to the dynamics in the training data. We find correspondences of the Koopman eigenvalues between the models. We see that the decaying dynamics in the Barents Sea region are roughly captured by the model trained on the entire northern hemisphere. The small differences in the learned dynamics are likely due to the fact that the model trained on data from the entire northern hemisphere is subject to more noise than the localized model trained only on data from the Barents Sea subregion. Further investigation into using information from the localized models to improve the global model is currently being conducted. This is part of our ongoing work on improving the robustness of the Koopman models.","title":"Additional Climate Variables as Observables"},{"location":"Results/next_steps/","text":"Preliminary Results Next Steps We plan to continue development and analysis of robustness and accuracy metrics to further demonstrate the relevance of Koopman modeling of climate systems. We will also investigate the inclusion of additional climate variables (top-of-atmosphere and surface heat fluxes). Early summaries of causal models using the updated FKPM will come as well, allowing for further validation of the approach and the relevance of the produced models.","title":"Next Steps"},{"location":"Results/next_steps/#preliminary-results","text":"","title":"Preliminary Results"},{"location":"Results/next_steps/#next-steps","text":"We plan to continue development and analysis of robustness and accuracy metrics to further demonstrate the relevance of Koopman modeling of climate systems. We will also investigate the inclusion of additional climate variables (top-of-atmosphere and surface heat fluxes). Early summaries of causal models using the updated FKPM will come as well, allowing for further validation of the approach and the relevance of the produced models.","title":"Next Steps"},{"location":"Results/results_Milestone_3/","text":"Preliminary results from Simple Fast Koopman Proxy Model We\u2019re initially investigating the simulation data from the CESM1 Large Ensemble Community Project. Due to the highly chaotic dynamics of the climate model, small differences in initial conditions lead to vastly different behaviors. The ensemble project simulates CESM1 using a constant set of parameters and forcing inputs across all runs. The only difference between different ensemble members are perturbations (at round-off error magnitude) on the initial conditions. One benefit to the Koopman Mode Decomposition is that while these perturbations can lead to large differences in observable values at a given time-step, they should be treated as noise from the perspective of FKPM. We expect to maintain good accuracy and to get very similar coefficients for the FKPM trained across each of the ensembles. All results shown here are of a FKPM that only models the dynamics of the Sea Ice levels provided as output from the full CESM1 simulation. This means that we cannot extract causality or execute any what-if analyses outside of self-coupling of sea-ice concentration to itself, but those external forcing functions are represented intrinsically in the output of the CESM1 simulation output. Before moving to simulations from the CESM1 Large Ensemble Community Project, we perform KMD on a control run that uses constant forcing inputs (instead of the observed historical forcing values). The mean and annual Koopman modes are shown in Figure 1. These control simulations do not rely on historical data for forcing inputs and can therefore be simulated for long time horizons. Figure 1: Mean (left) and annual (right) Koopman modes obtained from performing KMD on a control run from 1979-2021 in the northern hemisphere. Figure 2 shows the mean and annual Koopman modes obtained from performing KMD on CESM1 simulation runs #001 and #105 with historical forcing. Koopman models are produced for each ensemble member, while there are some differences in the spectral modes presented, the overall picture is quite similar between the two. The standard deviations of the mean and annual modes are shown in Figure 3. We can see where bias from a specific ensemble selection may come in while training an FKPM. Figure 2: Mean (left) and annual (right) Koopman modes obtained from performing KMD on CESM1 simulations with historical forcing from 1979-2021 in the northern hemisphere. Two members were selected to show typical differences: the top plots show results from member #001 and the bottom plots show results from member #105. Figure 3: Standard deviation of the mean (left) and annual (right) Koopman modes obtained from CESM1 simulations with historical forcing between all ensemble members. Next steps for this preliminary analysis include: comparing FKPMs trained on observational data to CESM1 simulation data which should give a handle on how much we can learn about dynamics that are not explicitly modeled in the CESM1 simulation and thus how much accuracy/precision gain we may get by training the HKCM. Then we\u2019ll begin analyses that include more observable features and begin some what-if experimentation with ensemble of FKPMs.","title":"results Milestone 3"},{"location":"Results/results_Milestone_3/#preliminary-results-from-simple-fast-koopman-proxy-model","text":"We\u2019re initially investigating the simulation data from the CESM1 Large Ensemble Community Project. Due to the highly chaotic dynamics of the climate model, small differences in initial conditions lead to vastly different behaviors. The ensemble project simulates CESM1 using a constant set of parameters and forcing inputs across all runs. The only difference between different ensemble members are perturbations (at round-off error magnitude) on the initial conditions. One benefit to the Koopman Mode Decomposition is that while these perturbations can lead to large differences in observable values at a given time-step, they should be treated as noise from the perspective of FKPM. We expect to maintain good accuracy and to get very similar coefficients for the FKPM trained across each of the ensembles. All results shown here are of a FKPM that only models the dynamics of the Sea Ice levels provided as output from the full CESM1 simulation. This means that we cannot extract causality or execute any what-if analyses outside of self-coupling of sea-ice concentration to itself, but those external forcing functions are represented intrinsically in the output of the CESM1 simulation output. Before moving to simulations from the CESM1 Large Ensemble Community Project, we perform KMD on a control run that uses constant forcing inputs (instead of the observed historical forcing values). The mean and annual Koopman modes are shown in Figure 1. These control simulations do not rely on historical data for forcing inputs and can therefore be simulated for long time horizons. Figure 1: Mean (left) and annual (right) Koopman modes obtained from performing KMD on a control run from 1979-2021 in the northern hemisphere. Figure 2 shows the mean and annual Koopman modes obtained from performing KMD on CESM1 simulation runs #001 and #105 with historical forcing. Koopman models are produced for each ensemble member, while there are some differences in the spectral modes presented, the overall picture is quite similar between the two. The standard deviations of the mean and annual modes are shown in Figure 3. We can see where bias from a specific ensemble selection may come in while training an FKPM. Figure 2: Mean (left) and annual (right) Koopman modes obtained from performing KMD on CESM1 simulations with historical forcing from 1979-2021 in the northern hemisphere. Two members were selected to show typical differences: the top plots show results from member #001 and the bottom plots show results from member #105. Figure 3: Standard deviation of the mean (left) and annual (right) Koopman modes obtained from CESM1 simulations with historical forcing between all ensemble members. Next steps for this preliminary analysis include: comparing FKPMs trained on observational data to CESM1 simulation data which should give a handle on how much we can learn about dynamics that are not explicitly modeled in the CESM1 simulation and thus how much accuracy/precision gain we may get by training the HKCM. Then we\u2019ll begin analyses that include more observable features and begin some what-if experimentation with ensemble of FKPMs.","title":"Preliminary results from Simple Fast Koopman Proxy Model"},{"location":"Tutorials/configure_haiku/","text":"Configuring HAIKU on your System Cloning HAIKU First, clone the github HAIKU repository: git clone https://github.com/BAE-Systems-HAIKU/HAIKU.git HAIKU cd HAIKU Setting up python env Next setup the python environment: HAIKU expects a Linux environment running Python 3.8 or higher. We recommend using a Python Virtual Environment to isolate HAIKU dependencies from the rest of your system. You can use your favorite approach to set this up, but we provide instructions assuming python venv and pip as environment and package managers. To create this environment, run python3.8 -m venv ./haiku-venv . Then, activate the virtual environment with source ./haiku-venv/bin/activate . There are Python library dependencies users need to download before running the system. To do so, users should use pip in conjunction with the requirements.txt file found in the base directory of the public GitHub repository. The command to install the Python dependencies is: pip install -r requirements.txt Finally, the system PYTHONPATH environment variable must include the root directory of the haiku software. For example, if this codebase was located at /home/test/core/haiku , it could be added to the system PYTHONPATH with the following command: export PYTHONPATH=$PYTHONPATH:/home/test/core/haiku . This command can be added to ~/.bashrc on a Linux system so that it is applied automatically whenever a terminal is launched. Installing CDO (optional) We execute this data alignment with the Climate Data Operators (cd0) tool available here . This is required only if one needs to regrid the .nc climate data. This code must be compiled before it can be run and requires the c++14 standard. Additionally, the data is in netCDF format, which means cdo must be compiled with netDCF which itself requires hdf5 . To accomplish the remapping from stereographic projection cdo requires proj (which requires a new enough version of sqlite to compile. make_install_cdo.sh should handle downloading and compiling all the appropriate files as well as executing the appropriate regridding of all netCDF files. CDO requires an input parameter file to specific details of the target grid. The required target parameter file is given in configs/target_grid.txt . The NSIDC sea ice concentration data V4 has an underspecified grid defined in its .nc data file, so we also include the fully defined northern hemisphere and southern hemisphere grid definition for the stereographic grid ( configs/nh_icefrac_grid.txt ) and southern hemisphere ( configs/sh_icefrac_grid.txt ). The data_downloader.sh script will automatically regrid the NSIDC data. On Ubuntu 20.04, this binary can be downloaded directly from the package repositories with apt install cdo . If you prefer, the binary can be downloaded directly from the website linked above. If choosing this route, ensure the downloaded binary is placed on your system path (e.g., in /usr/bin ). HAIKU expects a globally accesible CDO binary. Data Download The data currently used in this system is described in detail in HAIKU data . We include a data_downloader.sh that automates downloading of all the CESM1 (simulation) data and some of the observation/reanalysis data. Unfortunately, much of the reanalysis data must be downloaded by hand, but descriptions of exactly what parameters to use to download the data are listed in the data_downloader.sh script. We will release a set of preprocessed data in the near future to reduce user effort and avoid possible issues in configuring the data, but will also maintain description of the full process for reproduceability. NCAR data We extract several variables from the NCAR getway hosted here . This data is free to download, but required registering a user account first. With a user account, an API Token is provided to access data hosted by NCAR/earthsystemsgrid.org. You can access this token by browsing to \"Account Home\" (listed under your username). This API token must be provided to data_downloader.sh as the single argument. NSIDC data The NSIDC data are hosted here covering 1987-present at a monthly temporal resolution. This data is hosted on an ftp server and no additional login information is required to download it. However, the coordinate system of the NSIDC data do not align with the CESM1 data and currently must be converted to latitude/longitude to make a direct comparison to the CESM1 output.","title":"Configure HAIKU"},{"location":"Tutorials/configure_haiku/#configuring-haiku-on-your-system","text":"","title":"Configuring HAIKU on your System"},{"location":"Tutorials/configure_haiku/#cloning-haiku","text":"First, clone the github HAIKU repository: git clone https://github.com/BAE-Systems-HAIKU/HAIKU.git HAIKU cd HAIKU","title":"Cloning HAIKU"},{"location":"Tutorials/configure_haiku/#setting-up-python-env","text":"Next setup the python environment: HAIKU expects a Linux environment running Python 3.8 or higher. We recommend using a Python Virtual Environment to isolate HAIKU dependencies from the rest of your system. You can use your favorite approach to set this up, but we provide instructions assuming python venv and pip as environment and package managers. To create this environment, run python3.8 -m venv ./haiku-venv . Then, activate the virtual environment with source ./haiku-venv/bin/activate . There are Python library dependencies users need to download before running the system. To do so, users should use pip in conjunction with the requirements.txt file found in the base directory of the public GitHub repository. The command to install the Python dependencies is: pip install -r requirements.txt Finally, the system PYTHONPATH environment variable must include the root directory of the haiku software. For example, if this codebase was located at /home/test/core/haiku , it could be added to the system PYTHONPATH with the following command: export PYTHONPATH=$PYTHONPATH:/home/test/core/haiku . This command can be added to ~/.bashrc on a Linux system so that it is applied automatically whenever a terminal is launched.","title":"Setting up python env"},{"location":"Tutorials/configure_haiku/#installing-cdo-optional","text":"We execute this data alignment with the Climate Data Operators (cd0) tool available here . This is required only if one needs to regrid the .nc climate data. This code must be compiled before it can be run and requires the c++14 standard. Additionally, the data is in netCDF format, which means cdo must be compiled with netDCF which itself requires hdf5 . To accomplish the remapping from stereographic projection cdo requires proj (which requires a new enough version of sqlite to compile. make_install_cdo.sh should handle downloading and compiling all the appropriate files as well as executing the appropriate regridding of all netCDF files. CDO requires an input parameter file to specific details of the target grid. The required target parameter file is given in configs/target_grid.txt . The NSIDC sea ice concentration data V4 has an underspecified grid defined in its .nc data file, so we also include the fully defined northern hemisphere and southern hemisphere grid definition for the stereographic grid ( configs/nh_icefrac_grid.txt ) and southern hemisphere ( configs/sh_icefrac_grid.txt ). The data_downloader.sh script will automatically regrid the NSIDC data. On Ubuntu 20.04, this binary can be downloaded directly from the package repositories with apt install cdo . If you prefer, the binary can be downloaded directly from the website linked above. If choosing this route, ensure the downloaded binary is placed on your system path (e.g., in /usr/bin ). HAIKU expects a globally accesible CDO binary.","title":"Installing CDO (optional)"},{"location":"Tutorials/configure_haiku/#data-download","text":"The data currently used in this system is described in detail in HAIKU data . We include a data_downloader.sh that automates downloading of all the CESM1 (simulation) data and some of the observation/reanalysis data. Unfortunately, much of the reanalysis data must be downloaded by hand, but descriptions of exactly what parameters to use to download the data are listed in the data_downloader.sh script. We will release a set of preprocessed data in the near future to reduce user effort and avoid possible issues in configuring the data, but will also maintain description of the full process for reproduceability.","title":"Data Download"},{"location":"Tutorials/configure_haiku/#ncar-data","text":"We extract several variables from the NCAR getway hosted here . This data is free to download, but required registering a user account first. With a user account, an API Token is provided to access data hosted by NCAR/earthsystemsgrid.org. You can access this token by browsing to \"Account Home\" (listed under your username). This API token must be provided to data_downloader.sh as the single argument.","title":"NCAR data"},{"location":"Tutorials/configure_haiku/#nsidc-data","text":"The NSIDC data are hosted here covering 1987-present at a monthly temporal resolution. This data is hosted on an ftp server and no additional login information is required to download it. However, the coordinate system of the NSIDC data do not align with the CESM1 data and currently must be converted to latitude/longitude to make a direct comparison to the CESM1 output.","title":"NSIDC data"},{"location":"Tutorials/quickstart/","text":"Quickstart Quickstart: set up HAIKU and python env Thesteps below are explained in full detail in Configure HAIKU git clone https://github.com/BAE-Systems-HAIKU/HAIKU.git HAIKU cd HAIKU python3.8 -m venv ./haiku-venv source ./haiku-venv/bin/activate pip install -r requirements.txt #export PYTHONPATH=$PYTHONPATH:/your/absolute/path/to/HAIKU Quickstart: Data Download and Regridding Install CDO: bash make_install_cdo.sh Navigate to NCAR User API token to copy your API token (you will need to make and account/login) bash data_downloader.sh pasted_api_token to download and preprocess the relevant data. The resultant files are split into the data/CESM1/ and data/NSIDC/ directories with subdirectories related to the data frequency and variables of interest are ready to be ingested by the HAIKU code data_downloader.sh will also spit out text describing urls and rough instructions for any additional observational/reanalysis data that we've used in our analysis Quickstart: Training a Koopman Model Once a user has downloaded either the CESM or NSIDC datasets, they can use HAIKU to train a Koopman Model. The following steps outline how to do so: Copy the configs/example_config.yml file Update the new configuration file appropriately for your environment Data directories can contain either CESM or NSIDC dataset files Specifying data directories containing different dataset types (e.g., ICEFRAC and SST) will result in a model combining the two variable types Run python scripts/train.py path_to_configuration_file System output will be directed to the log file specified in the configuration file This will produce some plots related to the Koopman models that was trained (showing eigenvalues and eigenfunctions) The generated model can then be operated on using the prediction and plotting modules Examples for using the Koopman models are located in scripts/plotting python scripts/predict.py path_to_configuration_file.yaml trained_koopman_model_file.pkl YYYYMM01 path_to_output_directory This will produce a set of diagnostic plots related to accuracy of the Koopman model over the training data window extended to the data listed (YYYYMM01: 20201201 for instance)","title":"Quickstart"},{"location":"Tutorials/quickstart/#quickstart","text":"","title":"Quickstart"},{"location":"Tutorials/quickstart/#quickstart-set-up-haiku-and-python-env","text":"Thesteps below are explained in full detail in Configure HAIKU git clone https://github.com/BAE-Systems-HAIKU/HAIKU.git HAIKU cd HAIKU python3.8 -m venv ./haiku-venv source ./haiku-venv/bin/activate pip install -r requirements.txt #export PYTHONPATH=$PYTHONPATH:/your/absolute/path/to/HAIKU","title":"Quickstart: set up HAIKU and python env"},{"location":"Tutorials/quickstart/#quickstart-data-download-and-regridding","text":"Install CDO: bash make_install_cdo.sh Navigate to NCAR User API token to copy your API token (you will need to make and account/login) bash data_downloader.sh pasted_api_token to download and preprocess the relevant data. The resultant files are split into the data/CESM1/ and data/NSIDC/ directories with subdirectories related to the data frequency and variables of interest are ready to be ingested by the HAIKU code data_downloader.sh will also spit out text describing urls and rough instructions for any additional observational/reanalysis data that we've used in our analysis","title":"Quickstart: Data Download and Regridding"},{"location":"Tutorials/quickstart/#quickstart-training-a-koopman-model","text":"Once a user has downloaded either the CESM or NSIDC datasets, they can use HAIKU to train a Koopman Model. The following steps outline how to do so: Copy the configs/example_config.yml file Update the new configuration file appropriately for your environment Data directories can contain either CESM or NSIDC dataset files Specifying data directories containing different dataset types (e.g., ICEFRAC and SST) will result in a model combining the two variable types Run python scripts/train.py path_to_configuration_file System output will be directed to the log file specified in the configuration file This will produce some plots related to the Koopman models that was trained (showing eigenvalues and eigenfunctions) The generated model can then be operated on using the prediction and plotting modules Examples for using the Koopman models are located in scripts/plotting python scripts/predict.py path_to_configuration_file.yaml trained_koopman_model_file.pkl YYYYMM01 path_to_output_directory This will produce a set of diagnostic plots related to accuracy of the Koopman model over the training data window extended to the data listed (YYYYMM01: 20201201 for instance)","title":"Quickstart: Training a Koopman Model"}]}